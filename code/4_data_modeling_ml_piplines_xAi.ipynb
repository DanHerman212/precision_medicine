{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# common libraries\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import helper \n",
    "import sklearn\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# machine learning libraries    \n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier   \n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "import pydotplus    \n",
    "from IPython.display import Image\n",
    "from six import StringIO\n",
    "from sklearn.tree import export_graphviz\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read csv file\n",
    "data = pd.read_csv('../data/new_merged_data.csv')\n",
    "\n",
    "print('Shape of dataframe prior to modeling:', data.shape)\n",
    "display(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_names = [\n",
    " #'Propoxyphene',\n",
    " #'Amphetamines',\n",
    " #'Cannabinoids',\n",
    " #'Benzodiazepines',\n",
    " #'MMethadone',\n",
    " #'Oxycodone',\n",
    " #'Cocaine',\n",
    " #'Methamphetamine',\n",
    " 'Opiate300'\n",
    " ]\n",
    "\n",
    "# define prefix parameter and call helper function to build the dataset\n",
    "prefix = 'test_'\n",
    "tests = helper.feature_selection(data, prefix, test_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review Surveys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comment out the drugs you don't want to include\n",
    "survey_names = [\n",
    "#'cannabis',\n",
    "#'cocaine',\n",
    "#'alcohol',\n",
    "#'oxycodone',\n",
    "#'mmethadone',\n",
    "#'amphetamine',\n",
    "#'methamphetamine',\n",
    "'opiates',\n",
    "#'benzodiazepines'\n",
    "]\n",
    "\n",
    "# call helper function to build  the dataset\n",
    "prefix = 'survey_'\n",
    "surveys = helper.feature_selection(data, prefix, survey_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dataframe for Baseline Numeric Predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# medication data, included up to week 4\n",
    "group = 'meds'\n",
    "columns = 10\n",
    "meds = data[[col for col in data.columns if group in col]].iloc[:, :columns]\n",
    "\n",
    "# clinical opiate withdrawal scale -  included for assessment\n",
    "cows = data[[col for col in data.columns if 'cows' in col]]\n",
    "\n",
    "# rbs - risk behavior survey data\n",
    "rbs = data[[col for col in data.columns if 'rbs' in col]]\n",
    "\n",
    "# combine all the dataframes\n",
    "num_df = pd.concat([tests, surveys, meds, cows, rbs], axis=1)\n",
    "print('Shape of numeric dataframe after feature selection:', num_df.shape)\n",
    "display(num_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review Categorical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# demographic features\n",
    "dem = data[['gender']]\n",
    "\n",
    "print('shape of the categorical df:',dem.shape)\n",
    "display(dem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assign Target Variable and Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assigne dropout variable\n",
    "dropout = data['dropout']\n",
    "\n",
    "# merge\n",
    "new_data = pd.concat([num_df, dem, dropout], axis=1)\n",
    "\n",
    "print('Shape of new datframe:',new_data.shape)\n",
    "display(new_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the Data to Dev and Test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# assign variables\n",
    "X, y = new_data.drop(columns='dropout'), new_data['dropout']\n",
    "\n",
    "# create the dev and test sets \n",
    "X_dev, X_test, y_dev, y_test = train_test_split(X, y, test_size=0.14, random_state=42)\n",
    "\n",
    "# create the train and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_dev, y_dev, test_size=0.16, random_state=42)\n",
    "\n",
    "# check the shape of the train, validation, and test sets\n",
    "print('Shape of X_train:', X_train.shape)\n",
    "print('Shape of X_val:', X_val.shape)\n",
    "print('Shape of X_test:', X_test.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review Patient Profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.iloc[22, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit A Decision Tree\n",
    "No hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier(max_depth=None, random_state=42)\n",
    "dt.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measure the Train and Test with C-Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_preds = dt.predict_proba(X_train)[:, 1]\n",
    "print(f\"Train C-Index: {helper.cindex(y_train.values, y_train_preds)}\")\n",
    "\n",
    "y_val_preds = dt.predict_proba(X_val)[:, 1]\n",
    "print(f\"Val C-Index: {helper.cindex(y_val.values, y_val_preds)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update Hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update hyperparam configuration\n",
    "dt_hyperparams = {\n",
    "    \n",
    "\n",
    "    \n",
    "    'criterion': 'gini', \n",
    "    'max_depth': 3, \n",
    "    'max_features': None, \n",
    "    'min_samples_leaf': 1, \n",
    "    'min_samples_split': 2\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's test the accuracy on a regularlized tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_reg = DecisionTreeClassifier(**dt_hyperparams, random_state=10)\n",
    "dt_reg.fit(X_train, y_train)\n",
    "\n",
    "y_train_preds = dt_reg.predict_proba(X_train)[:, 1]\n",
    "y_val_preds = dt_reg.predict_proba(X_val)[:, 1]\n",
    "print(f\"Train C-Index: {helper.cindex(y_train.values, y_train_preds)}\")\n",
    "print(f\"Val C-Index (expected > 0.6): {helper.cindex(y_val.values, y_val_preds)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## See how the data fits the tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot_data = StringIO()\n",
    "export_graphviz(dt_reg, feature_names=X_train.columns, out_file=dot_data,  \n",
    "                filled=True, rounded=True, proportion=True, special_characters=True,\n",
    "                impurity=False, class_names=['neg', 'pos'], precision=2)\n",
    "graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n",
    "Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Grid Search\n",
    "We will create a reusable function to perform grid search with any classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create custom grid search function\n",
    "def perform_grid_search(X_train, y_train, X_val, y_val, classifier, hyperparams):\n",
    "    \"\"\"\n",
    "    Perform grid search with cross-validation for a given classifier and hyperparameters.\n",
    "\n",
    "    Parameters:\n",
    "    - X_train: Training features dataframe.\n",
    "    - y_train: Training target series.\n",
    "    - X_val: Validation features dataframe.\n",
    "    - y_val: Validation target series.\n",
    "    - classifier: The classifier to use (e.g., XGBClassifier()).\n",
    "    - hyperparams: Dictionary of hyperparameters to search.\n",
    "\n",
    "    Returns:\n",
    "    - results_df: Pandas DataFrame containing the results of the grid search.\n",
    "    \"\"\"\n",
    "    # Define a custom scoring function for the C-index\n",
    "    def cindex_score(y_true, y_pred):\n",
    "        return helper.cindex(y_true, y_pred)\n",
    "\n",
    "    # Wrap the custom scoring function using make_scorer\n",
    "    cindex_scorer = make_scorer(cindex_score, greater_is_better=True)\n",
    "\n",
    "    # Set up GridSearchCV with cross-validation and custom scorer\n",
    "    grid_search = GridSearchCV(estimator=classifier, param_grid=hyperparams, cv=5, scoring=cindex_scorer, n_jobs=-1, verbose=1)\n",
    "\n",
    "    # Fit GridSearchCV to the training data\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # Retrieve the best parameters\n",
    "    best_params = grid_search.best_params_\n",
    "    \n",
    "    # Retrieve the best model\n",
    "    best_model = grid_search.best_estimator_\n",
    "\n",
    "    # Get the probability scores from the best model\n",
    "    y_val_preds = best_model.predict_proba(X_val)[:, 1]\n",
    "\n",
    "    # Calculate the C-index on the validation set\n",
    "    c_index = helper.cindex(y_val.values, y_val_preds)\n",
    "    \n",
    "    # Store the results in a DataFrame\n",
    "    results = pd.DataFrame(grid_search.cv_results_)\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Param Grid and loop through pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the classifiers and hyperparameters\n",
    "\n",
    "classifiers = {\n",
    "    'Logistic Regression': (LogisticRegression(), {\n",
    "        'C': [0.01, 0.1, 1, 10],\n",
    "        'penalty': ['l1', 'l2'],\n",
    "        'solver': ['liblinear']\n",
    "    }),\n",
    "    'Random Forest': (RandomForestClassifier(), {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [3, 4],\n",
    "        'min_samples_split': [2, 5, 10]\n",
    "    }),\n",
    "    'XGBoost': (XGBClassifier(), {\n",
    "        'max_depth': [3, 4],\n",
    "        'learning_rate': [0.001, 0.01, 0.1, 0.2],\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'reg_alpha': [0, 0.5, 1],\n",
    "        'reg_lambda': [0, 0.5, 1]\n",
    "    })\n",
    "}\n",
    "\n",
    "# store results in a dictionary\n",
    "results = {}\n",
    "\n",
    "# Perform grid search for each classifier\n",
    "for clf_name, (clf, params) in classifiers.items():\n",
    "    print(f\"Running grid search for {clf_name}...\")\n",
    "    results[clf_name] = perform_grid_search(X_train, y_train, X_val, y_val, clf, params)\n",
    "    print()\n",
    "\n",
    "\n",
    "# print the best results for each classifier\n",
    "for clf_name, _ in classifiers.items():\n",
    "    best_cindex = results[clf_name]['mean_test_score'].max()\n",
    "    best_params = results[clf_name]['params'][results[clf_name]['mean_test_score'].idxmax()]\n",
    "    print(f\"Best C-index for {clf_name}: {best_cindex:.4f}\")\n",
    "    print(f\"Best hyperparameters: {best_params}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run each classifier with the best params on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run each classifier with the best params on the test set\n",
    "for clf_name, (clf, params) in classifiers.items():\n",
    "    # Initialize the classifier\n",
    "    clf.set_params(**results[clf_name].loc[results[clf_name]['rank_test_score'] == 1, 'params'].iloc[0])\n",
    "\n",
    "    # Fit the model\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # Get the probability scores\n",
    "    y_test_preds = clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # Calculate the C-index\n",
    "    c_index = helper.cindex(y_test.values, y_test_preds)\n",
    "    print(f\"{clf_name} C-index for Test Set: {c_index:.2f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Confusion Matrix with Precision Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "# Assuming y_test contains the true labels and y_test_preds contains the predicted labels\n",
    "for clf_name, (clf, params) in classifiers.items():\n",
    "    # Initialize the classifier\n",
    "    clf.set_params(**results[clf_name].loc[results[clf_name]['rank_test_score'] == 1, 'params'].iloc[0])\n",
    "\n",
    "    # Fit the model\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # Get the predicted labels\n",
    "    y_test_preds = clf.predict(X_test)\n",
    "\n",
    "    # calculate recall\n",
    "    recall = recall_score(y_test, y_test_preds, pos_label=1.0)  # Adjust pos_label as needed\n",
    "\n",
    "    # Calculate precision\n",
    "    precision = precision_score(y_test, y_test_preds, pos_label=1.0)  # Adjust pos_label as needed\n",
    "    \n",
    "    # Plot the precision right before each confusion matrix\n",
    "    print(f'{clf_name} Precision Score: {precision:.2f}')\n",
    "    print(f'{clf_name} Recall Score: {recall:.2f}')\n",
    "    print()\n",
    "\n",
    "    # plot confusion matrix\n",
    "    helper.plot_confusion_matrix(y_test, y_test_preds, classes=['No Dropout', 'Dropout'],\n",
    "                          title=f'{clf_name} - Confusion matrix')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create classifier object\n",
    "xgb = classifiers['XGBoost'][0].set_params(**results['XGBoost'].loc[results['XGBoost']['rank_test_score'] == 1, 'params'].iloc[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call helper function to plot feature importance\n",
    "helper.plot_feature_importance(xgb, X_train, metric=\"gain\", num_features=19)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Shapley Values to Explain the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "ex = shap.Explainer(xgb.predict_proba, X_train, max_evals=700)\n",
    "shap_values = ex(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# waterfall plot for clas index 0 \n",
    "class_index = 1\n",
    "data_index = np.random.choice(shap_values.shape[0])\n",
    "\n",
    "print(f'Class index: {class_index}')\n",
    "print(f'Data index: {data_index}')\n",
    "\n",
    "shap.plots.waterfall(shap_values[data_index,:,class_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.waterfall(shap_values[50,:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_index = np.random.choice(shap_values.shape[0])\n",
    "class_index = 1\n",
    "print(f'Class index: {class_index}')\n",
    "print(f'Data index: {data_index}')\n",
    "shap.plots.waterfall(shap_values[data_index,:,class_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.initjs()\n",
    "\n",
    "class_index = 1\n",
    "data_index = np.random.choice(shap_values.shape[0])\n",
    "\n",
    "print(f'Class index: {class_index}')\n",
    "print(f'Data index: {data_index}')\n",
    "\n",
    "shap.plots.force(shap_values[data_index,:,class_index], matplotlib=True, show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_index = 1\n",
    "print(f'Class index: {class_index}')\n",
    "\n",
    "# shap beeswarm plot\n",
    "shap.plots.beeswarm(shap_values[:,:,class_index], max_display=15, show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract SHAP values for the dependence plot\n",
    "shap_values_array = shap_values.values\n",
    "\n",
    "# Create a dependence plot for a specific feature\n",
    "shap.dependence_plot('cows_postdose', shap_values_array[:,:,1], X_test, interaction_index='gender', show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Assuming you have the accuracies stored in the following dictionaries\n",
    "accuracies = {\n",
    "    'Logistic Regression': {\n",
    "        'train': 0.85,\n",
    "        'val': 0.80,\n",
    "        'test': 0.78\n",
    "    },\n",
    "    'Random Forest': {\n",
    "        'train': 0.90,\n",
    "        'val': 0.82,\n",
    "        'test': 0.80\n",
    "    },\n",
    "    'XGBoost': {\n",
    "        'train': 0.88,\n",
    "        'val': 0.83,\n",
    "        'test': 0.81\n",
    "    }\n",
    "}\n",
    "\n",
    "# Extract the data\n",
    "models = list(accuracies.keys())\n",
    "train_accuracies = [accuracies[model]['train'] for model in models]\n",
    "val_accuracies = [accuracies[model]['val'] for model in models]\n",
    "test_accuracies = [accuracies[model]['test'] for model in models]\n",
    "\n",
    "# Set up the bar plot\n",
    "x = np.arange(len(models))  # the label locations\n",
    "width = 0.2  # the width of the bars\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "rects1 = ax.bar(x - width, train_accuracies, width, label='Train')\n",
    "rects2 = ax.bar(x, val_accuracies, width, label='Validation')\n",
    "rects3 = ax.bar(x + width, test_accuracies, width, label='Test')\n",
    "\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "ax.set_xlabel('Models')\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_title('Accuracy by model and dataset')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(models)\n",
    "ax.legend()\n",
    "# put legend outside of plot\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "# Attach a text label above each bar in rects, displaying its height.\n",
    "def autolabel(rects):\n",
    "    \"\"\"Attach a text label above each bar in *rects*, displaying its height.\"\"\"\n",
    "    for rect in rects:\n",
    "        height = rect.get_height()\n",
    "        ax.annotate('{}'.format(height),\n",
    "                    xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "                    xytext=(0, 3),  # 3 points vertical offset\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom')\n",
    "\n",
    "autolabel(rects1)\n",
    "autolabel(rects2)\n",
    "autolabel(rects3)\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
