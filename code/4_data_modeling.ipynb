{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import helper \n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.datasets import make_classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Modeling Workflow\n",
    "The following workflow will be applied in building the data model:\n",
    "1.  Extract tests at assessment and weeks 0 - 4 for treatment<br>\n",
    "2.  Extract surveys at assessment and weeks 0, 4 for treatment\n",
    "3.  Extract Other Numeric Baseline Predictors, including:\n",
    "    - attendence\n",
    "    - medication dose\n",
    "    - clinical opiate withdrawal scale - score of 1 - 13, lower score shows lower withdrawal symptoms\n",
    "4.  Extract Catagorical Baseline Predictors, including:\n",
    "    - Demographics\n",
    "    - Substance Use Diagnosis\n",
    "    - Mental and Pysical Health History\n",
    "    - Physical Exam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read csv file\n",
    "data = pd.read_csv('../data/merged_data.csv')\n",
    "\n",
    "print('Shape of dataframe prior to modeling:', data.shape)\n",
    "display(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.loc[data['medication']==2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_names = [\n",
    " 'Propoxyphene',\n",
    " 'Amphetamines',\n",
    " 'Cannabinoids',\n",
    " 'Benzodiazepines',\n",
    " 'MMethadone',\n",
    " 'Oxycodone',\n",
    " 'Cocaine',\n",
    " 'Methamphetamine',\n",
    " 'Opiate300'\n",
    " ]\n",
    "\n",
    "# define prefix parameter and call helper function to build the dataset\n",
    "prefix = 'test_'\n",
    "tests = helper.feature_selection(data, prefix, test_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review Surveys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comment out the drugs you don't want to include\n",
    "survey_names = [\n",
    "'cannabis',\n",
    "'cocaine',\n",
    "'alcohol',\n",
    "'oxycodone',\n",
    "'mmethadone',\n",
    "'amphetamine',\n",
    "'methamphetamine',\n",
    "'opiates',\n",
    "'benzodiazepines'\n",
    "]\n",
    "\n",
    "# call helper function to build  the dataset\n",
    "prefix = 'survey_'\n",
    "surveys = helper.feature_selection(data, prefix, survey_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dataframe for Baseline Numeric Predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attendance data, included up to week 4\n",
    "#rsa = data[[col for col in data.columns if 'rsa' in col]].iloc[:, :5]\n",
    "\n",
    "# medication data, included up to week 4\n",
    "group = 'meds_buprenorphine'\n",
    "meds = data[[col for col in data.columns if group in col]].iloc[:, :5]\n",
    "\n",
    "# clinical opiate withdrawal scale -  included for baseline and week 0\n",
    "cows = data[[col for col in data.columns if 'cows' in col]]\n",
    "\n",
    "# create dataframe with numeric features\n",
    "num_df = pd.concat([tests, surveys, meds, cows], axis=1)\n",
    "\n",
    "print('Shape of numeric dataframe',num_df.shape)\n",
    "display(num_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review Categorical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# demographic data\n",
    "dem = data[[col for col in data.columns if col.startswith('dem_')]]\n",
    "#dem = pd.get_dummies(dem, dtype=int)\n",
    "# create df for diagnosis\n",
    "dsm = data[[col for col in data.columns if col.startswith('dsm_')]]\n",
    "#dsm = pd.get_dummies(dsm, dtype=int)\n",
    "# create df for medical history\n",
    "mdh = data[[col for col in data.columns if col.startswith('mdh_')]]\n",
    "#mdh = pd.get_dummies(mdh, dtype=int)\n",
    "\n",
    "# create df for physical exam\n",
    "pex = data[[col for col in data.columns if col.startswith('pex_')]]\n",
    "#pex = pd.get_dummies(pex, dtype=int)\n",
    "\n",
    "# concat cat features into cat_df\n",
    "cat_df = pd.concat([dem, dsm, mdh,pex], axis=1)\n",
    "print('shape of the categorical df:',cat_df.shape)\n",
    "display(cat_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We found some noisy data in this dataset\n",
    "There were two different imputations applied to dsm columns<br>\n",
    "Combine 'not_present' and 'not_evaluated' to 'not_evaluated'<br>\n",
    "There is a 0 value in `dem_gender` in 3 rows, we will delete them<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace values labeled 'not_present', with 'not_evaluated'\n",
    "cat_df = cat_df.replace('not_present', 'not_evaluated')\n",
    "\n",
    "# remove rows with 0 values\n",
    "cat_df.loc[cat_df.dem_gender!='0']\n",
    "\n",
    "print(cat_df.shape)\n",
    "display(cat_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encode cat_df\n",
    "cat_df = pd.get_dummies(cat_df, drop_first=True, dtype=float)\n",
    "\n",
    "print(cat_df.shape)\n",
    "display(cat_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assign Target Variable and Combine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assigne dropout variable\n",
    "dropout = data['dropout']\n",
    "\n",
    "# merge\n",
    "new_data = pd.concat([num_df, cat_df, dropout], axis=1)\n",
    "\n",
    "print('Shape of new datframe:',new_data.shape)\n",
    "display(new_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data.to_csv('../data/all_features.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_features = new_data.drop(columns=['dropout'])\n",
    "\n",
    "baseline_features.to_csv('../data/baseline_features_cols.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data.to_csv('../data/bupe_baseline_features.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# create train, test and eval set\n",
    "from fast_ml.model_development import train_valid_test_split\n",
    "train = 0.7\n",
    "test = 0.2\n",
    "validation = 0.1\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = train_valid_test_split(new_data,\n",
    "                      target='dropout',\n",
    "                      train_size=train,                        \n",
    "                      test_size=test,\n",
    "                      valid_size=validation)\n",
    "\n",
    "# print shape of all data sets\n",
    "print('Shape of X_train:', X_train.shape)\n",
    "print('Shape of X_val:', X_val.shape)\n",
    "print('Shape of X_test:', X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm_param_grid = {\n",
    "\t'learning_rate': [0.01, 0.1, 0.5, 0.9],\n",
    "\t'n_estimators': [200, 300, 400],\n",
    "    'max_depth': [3, 6, 9],\n",
    "\t'objective': ['binary:logistic']  # Specify binary logistic objective\n",
    "}\n",
    "\n",
    "gbm = xgb.XGBClassifier(use_label_encoder=False)  # XGBClassifier for classification tasks\n",
    "grid_auc = GridSearchCV(estimator=gbm, \n",
    "                        param_grid=gbm_param_grid, \n",
    "                        scoring='roc_auc', \n",
    "                        cv=5, \n",
    "                        verbose=1, \n",
    "                        return_train_score=True,\n",
    "                        n_jobs=-1)  # Use ROC AUC as the scoring metric\n",
    "\n",
    "grid_auc.fit(X_train, y_train)\n",
    "print(\"Best parameters found: \", grid_auc.best_params_)\n",
    "print(\"Highest ROC AUC found: \", grid_auc.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# refit the model with the best params\n",
    "best_params = grid_auc.best_params_\n",
    "\n",
    "# best model\n",
    "best_gbm = xgb.XGBClassifier(**best_params)\n",
    "\n",
    "# fit the model\n",
    "best_gbm.fit(X_train, y_train, eval_set=[(X_train,y_train),(X_val, y_val)], eval_metric=['auc','error'], verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the confusion matrix\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "y_pred = best_gbm.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=best_gbm.classes_)\n",
    "disp.plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve performance metrics\n",
    "results = best_gbm.evals_result()\n",
    "epochs = len(results['validation_0']['auc'])\n",
    "x_axis = range(0, epochs)\n",
    "\n",
    "# plot log loss and classification error \n",
    "fig, axs = plt.subplots(ncols=2, figsize=(18, 5))\n",
    "\n",
    "# plot log loss\n",
    "axs[0].plot(x_axis, results['validation_0']['error'], label='Train')\n",
    "axs[0].plot(x_axis, results['validation_1']['error'], label='Test')\n",
    "axs[0].legend()\n",
    "axs[0].set_ylabel('Error')\n",
    "axs[0].set_title('XGBoost Error', fontsize=16)\n",
    "\n",
    "# plot classification error\n",
    "axs[1].plot(x_axis, results['validation_0']['auc'], label='Train')\n",
    "axs[1].plot(x_axis, results['validation_1']['auc'], label='Test')\n",
    "axs[1].legend()\n",
    "axs[1].set_ylabel('AUC')\n",
    "axs[1].set_title('XGBoost AUC', fontsize=16)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions to evaluate the model\n",
    "y_pred_train = best_gbm.predict(X_train)\n",
    "y_pred_val = best_gbm.predict(X_val)\n",
    "y_pred_test = best_gbm.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create DF with Train, Val and Test metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Assuming you have your predictions as y_pred_train, y_pred_val, y_pred_test\n",
    "# and the true labels as y_train, y_val, y_test\n",
    "\n",
    "# Generate classification reports\n",
    "report_train = classification_report(y_train, y_pred_train, output_dict=True)\n",
    "report_val = classification_report(y_val, y_pred_val, output_dict=True)\n",
    "report_test = classification_report(y_test, y_pred_test, output_dict=True)\n",
    "\n",
    "# Create DataFrames from reports\n",
    "df_train = pd.DataFrame(report_train).transpose()\n",
    "df_val = pd.DataFrame(report_val).transpose()\n",
    "df_test = pd.DataFrame(report_test).transpose()\n",
    "\n",
    "# Add a column to distinguish between datasets\n",
    "df_train['dataset'] = 'Training'\n",
    "df_val['dataset'] = 'Validation'\n",
    "df_test['dataset'] = 'Test'\n",
    "\n",
    "# Concatenate all three DataFrames\n",
    "df_all = pd.concat([df_train, df_val, df_test], axis=0).reset_index()\n",
    "\n",
    "# Rename 'index' column to something more descriptive, like 'metric'\n",
    "df_all.rename(columns={'index': 'metric'}, inplace=True)\n",
    "\n",
    "# subset columns\n",
    "#df_all = df_all[['metric','precision','dataset']]\n",
    "\n",
    "# pivot the dataframe\n",
    "#df_all = df_all.pivot(index='dataset', columns='metric', values='precision').reset_index()\n",
    "\n",
    "# reorder rows, 1, 2, 0\n",
    "#df_all = df_all.reindex([1,2,0])\n",
    "\n",
    "classification_metrics = df_all.loc[\n",
    "                                    (df_all['metric']=='0.0') | \n",
    "                                    (df_all['metric']=='1.0') |\n",
    "                                    (df_all['metric']=='macro avg')    \n",
    "                                ]\n",
    "\n",
    "classification_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "\n",
    "# Assuming grid_auc.best_estimator_ is your trained XGBoost model\n",
    "model = grid_auc.best_estimator_\n",
    "\n",
    "metric='gain'\n",
    "plot = metric.capitalize().replace('_g', ' G')\n",
    "\n",
    "# Get feature importances and round them\n",
    "importances = model.get_booster().get_score(importance_type=metric)\n",
    "importances_rounded = {k: round(v, 2) for k, v in importances.items()}\n",
    "\n",
    "# Sort features by importance\n",
    "sorted_importances = sorted(importances_rounded.items(), key=lambda x: x[1], reverse=True)[:20]\n",
    "\n",
    "# Separate keys and values for plotting\n",
    "features, scores = zip(*sorted_importances)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.barh(range(len(scores)), scores, color='lightgreen')\n",
    "plt.yticks(range(len(scores)), features)\n",
    "plt.xlabel('F Score')\n",
    "plt.ylabel('Features')\n",
    "# annotate the values over the bars\n",
    "for i, v in enumerate(scores):\n",
    "    plt.text(v + 0.1, i - 0.1, str(v), color='black', va='center')\n",
    "plt.title(f'Feature Importance - {plot}', fontsize=16)\n",
    "plt.gca().invert_yaxis()  # Invert y-axis to have the most important feature on top\n",
    "# remove borders\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "plt.gca().spines['left'].set_visible(False)\n",
    "plt.gca().spines['bottom'].set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the first element of all rows in the tuple\n",
    "features = [x[0] for x in sorted_importances]\n",
    "\n",
    "final_predictors = new_data[features+['dropout']]\n",
    "\n",
    "print(final_predictors.shape)\n",
    "display(final_predictors)\n",
    "\n",
    "# save to csv\n",
    "final_predictors.to_csv('../data/final_predictors.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create train, test and eval set\n",
    "from fast_ml.model_development import train_valid_test_split\n",
    "train = 0.7\n",
    "test = 0.2\n",
    "validation = 0.1\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = train_valid_test_split(final_predictors,\n",
    "                      target='dropout',\n",
    "                      train_size=train,                        \n",
    "                      test_size=test,\n",
    "                      valid_size=validation)\n",
    "\n",
    "# print shape of all data sets\n",
    "print('Shape of X_train:', X_train.shape)\n",
    "print('Shape of X_val:', X_val.shape)\n",
    "print('Shape of X_test:', X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm_param_grid = {\n",
    "\t'learning_rate': [0.01, 0.1, 0.5, 0.9],\n",
    "\t'n_estimators': [200, 300, 400],\n",
    "    'max_depth': [3, 6, 9],\n",
    "\t'objective': ['binary:logistic']  # Specify binary logistic objective\n",
    "}\n",
    "\n",
    "gbm = xgb.XGBClassifier(use_label_encoder=False)  # XGBClassifier for classification tasks\n",
    "grid_auc = GridSearchCV(estimator=gbm, \n",
    "                        param_grid=gbm_param_grid, \n",
    "                        scoring='roc_auc', \n",
    "                        cv=5, \n",
    "                        verbose=1, \n",
    "                        return_train_score=True,\n",
    "                        n_jobs=-1)  # Use ROC AUC as the scoring metric\n",
    "\n",
    "grid_auc.fit(X_train, y_train)\n",
    "print(\"Best parameters found: \", grid_auc.best_params_)\n",
    "print(\"Highest ROC AUC found: \", grid_auc.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# refit the model with the best params\n",
    "best_params = grid_auc.best_params_\n",
    "\n",
    "# best model\n",
    "best_gbm = xgb.XGBClassifier(**best_params)\n",
    "\n",
    "# fit the model\n",
    "best_gbm.fit(X_train, y_train, eval_set=[(X_train,y_train),(X_val, y_val)], eval_metric=['auc','error'], verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions to evaluate the model\n",
    "y_pred_train = best_gbm.predict(X_train)\n",
    "y_pred_val = best_gbm.predict(X_val)\n",
    "y_pred_test = best_gbm.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the confusion matrix\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "y_pred = best_gbm.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=best_gbm.classes_)\n",
    "disp.plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve performance metrics\n",
    "results = best_gbm.evals_result()\n",
    "epochs = len(results['validation_0']['auc'])\n",
    "x_axis = range(0, epochs)\n",
    "\n",
    "# plot log loss and classification error \n",
    "fig, axs = plt.subplots(ncols=2, figsize=(18, 5))\n",
    "\n",
    "# plot log loss\n",
    "axs[0].plot(x_axis, results['validation_0']['error'], label='Train')\n",
    "axs[0].plot(x_axis, results['validation_1']['error'], label='Test')\n",
    "axs[0].legend()\n",
    "axs[0].set_ylabel('Error')\n",
    "axs[0].set_title('XGBoost Error', fontsize=16)\n",
    "\n",
    "# plot classification error\n",
    "axs[1].plot(x_axis, results['validation_0']['auc'], label='Train')\n",
    "axs[1].plot(x_axis, results['validation_1']['auc'], label='Test')\n",
    "axs[1].legend()\n",
    "axs[1].set_ylabel('AUC')\n",
    "axs[1].set_title('XGBoost AUC', fontsize=16)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Assuming you have your predictions as y_pred_train, y_pred_val, y_pred_test\n",
    "# and the true labels as y_train, y_val, y_test\n",
    "\n",
    "# Generate classification reports\n",
    "report_train = classification_report(y_train, y_pred_train, output_dict=True)\n",
    "report_val = classification_report(y_val, y_pred_val, output_dict=True)\n",
    "report_test = classification_report(y_test, y_pred_test, output_dict=True)\n",
    "\n",
    "# Create DataFrames from reports\n",
    "df_train = pd.DataFrame(report_train).transpose()\n",
    "df_val = pd.DataFrame(report_val).transpose()\n",
    "df_test = pd.DataFrame(report_test).transpose()\n",
    "\n",
    "# Add a column to distinguish between datasets\n",
    "df_train['dataset'] = 'Training'\n",
    "df_val['dataset'] = 'Validation'\n",
    "df_test['dataset'] = 'Test'\n",
    "\n",
    "# Concatenate all three DataFrames\n",
    "df_all = pd.concat([df_train, df_val, df_test], axis=0).reset_index()\n",
    "\n",
    "# Rename 'index' column to something more descriptive, like 'metric'\n",
    "df_all.rename(columns={'index': 'metric'}, inplace=True)\n",
    "\n",
    "# subset columns\n",
    "df_all = df_all[['metric','precision','dataset']]\n",
    "\n",
    "# pivot the dataframe\n",
    "df_all = df_all.pivot(index='dataset', columns='metric', values='precision').reset_index()\n",
    "\n",
    "# reorder rows, 1, 2, 0\n",
    "df_all = df_all.reindex([1,2,0])\n",
    "\n",
    "df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "\n",
    "# Assuming grid_auc.best_estimator_ is your trained XGBoost model\n",
    "model = grid_auc.best_estimator_\n",
    "\n",
    "metric='gain'\n",
    "plot = metric.capitalize().replace('_g', ' G')\n",
    "\n",
    "# Get feature importances and round them\n",
    "importances = model.get_booster().get_score(importance_type=metric)\n",
    "importances_rounded = {k: round(v, 2) for k, v in importances.items()}\n",
    "\n",
    "# Sort features by importance\n",
    "sorted_importances = sorted(importances_rounded.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Separate keys and values for plotting\n",
    "features, scores = zip(*sorted_importances)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.barh(range(len(scores)), scores, color='lightgreen')\n",
    "plt.yticks(range(len(scores)), features)\n",
    "plt.xlabel('F Score')\n",
    "plt.ylabel('Features')\n",
    "# annotate the values over the bars\n",
    "for i, v in enumerate(scores):\n",
    "    plt.text(v + 0.1, i - 0.1, str(v), color='black', va='center')\n",
    "plt.title(f'Feature Importance - {plot}', fontsize=16)\n",
    "plt.gca().invert_yaxis()  # Invert y-axis to have the most important feature on top\n",
    "# remove borders\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "plt.gca().spines['left'].set_visible(False)\n",
    "plt.gca().spines['bottom'].set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a DF with all the trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming `grid_auc` is your trained GridSearchCV object with an XGBClassifier\n",
    "best_model = grid_auc.best_estimator_\n",
    "\n",
    "# Extract the Booster\n",
    "booster = best_model.get_booster()\n",
    "\n",
    "# Convert the trees to a DataFrame\n",
    "trees_df = booster.trees_to_dataframe().iloc[:, :10]\n",
    "\n",
    "trees_df = trees_df.sort_values(by='Gain', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trees_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot a Specific Estimator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot tree with to_graphviz() sideways\n",
    "graph = xgb.to_graphviz(booster, num_trees=1)\n",
    "graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Test Set with Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming `model` is your trained model and `X_test`, `y_test` are your test datasets\n",
    "predictions = best_gbm.predict(X_test)\n",
    "\n",
    "# Convert X_test to a DataFrame if it's not already\n",
    "X_test_df = pd.DataFrame(X_test, columns=X_test.columns)\n",
    "\n",
    "# Add predictions and actual labels to the DataFrame\n",
    "X_test_df['Prediction'] = predictions\n",
    "X_test_df['Actual'] = y_test\n",
    "\n",
    "# Function to classify each prediction\n",
    "def classify_prediction(row):\n",
    "\tif row['Prediction'] == row['Actual']:\n",
    "\t\treturn 'TP' if row['Prediction'] == 1 else 'TN'\n",
    "\telse:\n",
    "\t\treturn 'FP' if row['Prediction'] == 1 else 'FN'\n",
    "\n",
    "# Apply the function to classify predictions\n",
    "X_test_df['Classification'] = X_test_df.apply(classify_prediction, axis=1)\n",
    "\n",
    "# Now X_test_df includes the test data, predictions, actual labels, and classification (TP, TN, FP, FN)\n",
    "test_set = X_test_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
