{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning Strategy\n",
    "For this project, we will use the CTN0027 Dataset from the clinical trial network.<br>\n",
    "The dataset and all it's documentation are avaialble at the following website:<br>\n",
    "https://datashare.nida.nih.gov/study/nida-ctn-0027<br>\n",
    "\n",
    "There are a few challenges to highlight that should be considered in the cleaning approach:\n",
    "- Large de-identified dataset must be manually labeled, very time consuming and prone to errors\n",
    "- High dimension data - Requires bespoke transformations to fit into machine learning models\n",
    "\n",
    "## Tables to be cleaned\n",
    "| File Name | Table Name | Variable |Description | Process Applied |\n",
    "| :--- | :--- | :--- | :--- | :--- |\n",
    "| T_FRRSA.csv | Research Session Attendance|RSA |Records attendence for each week of treatment | Clean, Flatten, Feature Extraction, Merge |\n",
    "| T_FRDEM.csv | Demographics|DEM |Sex, Ethnicity, Race | Clean, Merge |\n",
    "| T_FRUDSAB.csv | Urine Drug Screen| UDS  |Drug test for 8 different drug classes, taken weekly for 24 weeks | Clean, Flatten, Feature Extraction, Merge |\n",
    "| T_FRDSM.csv | DSM-IV Diagnosis|DSM |Tracks clinical diagnosis for substance use disorder, in accordance with DSM guidelines| Clean, Merge |\n",
    "| T_FRMDH.csv | Medical and Psychiatric History|MDH |Tracks medical and psychiatric history of 24 different Conditions| Clean, Merge |\n",
    "| T_FRPEX.csv | Physical Exam|PEX |Tracks the appearance and condition of patients for 12 different physical observations| Clean, Merge |\n",
    "| T_FRPBC_BL.csv | Pregnancy and Birth Control|PBC |Pregnancy test taken once per month on weeks 0, 4, 8, 12, 16, 20, 24| Clean, Merge |\n",
    "| T_FRTFB.csv | Timeline Follow Back Survey|TFB |Surveys for self reported drug use, collected every 4 weeks, includes previous 30 days of use ot week 0, 4, 8, 12, 16, 20, 24| Clean, Aggregate, Flatten, Merge |\n",
    "|T_FRDOS.csv | Dose Record |DOS |Records the medication, averge weekly dose and week of treatment| Clean, Aggregate, Feature Extraction, Flatten, Merge |\n",
    "\n",
    "\n",
    "## Data Cleaning Process\n",
    "We will try to keep things simple and employ a process driven by reusable functions<br>\n",
    "to **improve data quality**, **reduce time to market** and **reducing human error**.<br>\n",
    "<br> \n",
    "For each table we will follow the following steps:<br>\n",
    "1. Load the data\n",
    "2. Identify columns that require labels\n",
    "3. Apply labels to columns\n",
    "4. Drop columns that are not needed\n",
    "5. Create imputation strategy for missing values\n",
    "5. Apply transformations to values where required\n",
    "3. Feature Engineering (if necessary)\n",
    "4. Flatten Dataframes (encode week of treatment into columns, where applicable)\n",
    "4. Merge with other tables\n",
    "\n",
    "## List of Reusable Functions\n",
    "| Name of Function | Description | \n",
    "| ---------------- | ----------- |\n",
    "| clean_df | Clean the given DataFrame by dropping unnecessary columns, renaming columns, and reordering columns. |\n",
    "| flatten_dataframe | This function creates features by combining the VISIT column with the clinical datapoint (see example below).  The goal is to reduce individual rows per patient.  The data currently presents 25 rows per patient (for each week of treatment), which won't work for machine learning.  The model will only accept one row per patient, so we must encode all the clinical data into columns.  We will tranform the data by creating a separate dataframe for each week of treatment.  We will encode the week of treatment into the columns in each dataframe and then merge them together to form a high quality dataset, with granular level treatment data, that should help improve machine learning model accuracy.  This is a complex transformation, but justified for the incremental improvement to machine learning accuracy |\n",
    "| merge_dfs | Merge the given list of DataFrames into one DataFrame. |\n",
    "| uds_features | Creates 4 new features which are metrics used to measure outcomes from opiate test data. |\n",
    "| med_features | Creates 2 new features for medication dose to enrich dataset and improve accuracy in machine learning|\n",
    "\n",
    "### Example of How Flattening Works\n",
    "![flatten](../images/flatten.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # data manipulation library\n",
    "import numpy as np # numerical computing library\n",
    "import matplotlib.pyplot as plt # data visualization library\n",
    "import seaborn as sns # advanced data visualization library\n",
    "import helper # custom fuctions I created to clean and plot data\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Data\n",
    "We will load 10 files from the de-identified dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rsa shape: (27029, 12)\n",
      "dem shape: (1920, 43)\n",
      "uds shape: (24930, 66)\n",
      "dsm shape: (1889, 26)\n",
      "mdh shape: (1869, 89)\n",
      "pex shape: (2779, 33)\n",
      "pbc shape: (2691, 20)\n",
      "tfb shape: (100518, 56)\n",
      "dos shape: (160908, 19)\n"
     ]
    }
   ],
   "source": [
    "# define parameters to load data\n",
    "\n",
    "# define the path to the data\n",
    "data_path = '../unlabeled_data/'\n",
    "\n",
    "# define the names of the files to load\n",
    "file_names = ['T_FRRSA.csv', 'T_FRDEM.csv','T_FRUDSAB.csv',\n",
    "              'T_FRDSM.csv','T_FRMDH.csv','T_FRPEX.csv',\n",
    "              'T_FRPBC.csv','T_FRTFB.csv','T_FRDOS.csv']\n",
    "\n",
    "# define the names of the variables for the dataframes\n",
    "variables = ['rsa', 'dem', 'uds', 'dsm', 'mdh', 'pex', \n",
    "             'pbc', 'tfb', 'dos']\n",
    "\n",
    "# create a loop to iterate through the files and load them into the notebook\n",
    "for file_name, variable in zip(file_names, variables):\n",
    "        globals()[variable] = pd.read_csv(data_path + file_name)\n",
    "        print(f\"{variable} shape: {globals()[variable].shape}\") # print the shape of the dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform Attendence Table\n",
    "- This table establishes the patient population and will serve as the primary table\n",
    "- All subsequent tables will use a LEFT JOIN to add clinical data as columns to each patient ID\n",
    "- This table requires feature engineering for `attendance` and `dropout` variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24217, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patdeid</th>\n",
       "      <th>VISIT</th>\n",
       "      <th>attendance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27022</th>\n",
       "      <td>1931</td>\n",
       "      <td>23</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27023</th>\n",
       "      <td>1931</td>\n",
       "      <td>24</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27026</th>\n",
       "      <td>1932</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27027</th>\n",
       "      <td>1933</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27028</th>\n",
       "      <td>1934</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24217 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       patdeid  VISIT  attendance\n",
       "0            1      0         1.0\n",
       "2            1      1         1.0\n",
       "3            1      2         1.0\n",
       "4            1      3         1.0\n",
       "5            1      4         1.0\n",
       "...        ...    ...         ...\n",
       "27022     1931     23         0.0\n",
       "27023     1931     24         1.0\n",
       "27026     1932      0         1.0\n",
       "27027     1933      0         1.0\n",
       "27028     1934      0         1.0\n",
       "\n",
       "[24217 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# we will define the columns and labels that we need for each df and then transform the data\n",
    "\n",
    "# set parameters for transformation\n",
    "rsa_cols = ['patdeid','VISIT','RSA001']\n",
    "rsa_labels = {'RSA001':'attendance'}\n",
    "\n",
    "# the helper function will transform the data\n",
    "rsa = helper.clean_df(rsa, rsa_cols, rsa_labels)\n",
    "\n",
    "# fill nulls with 0, marking no attendance\n",
    "rsa['attendance'] = rsa['attendance'].fillna(0)\n",
    "\n",
    "# remove the followup visits from the main clinical data weeks 0 - 24\n",
    "rsa = rsa[~rsa['VISIT'].isin([28, 32])]\n",
    "\n",
    "# remove duplicate rows\n",
    "rsa = rsa.drop_duplicates(subset=['patdeid', 'VISIT'], keep='first')\n",
    "\n",
    "# observe shape and sample 5 observations\n",
    "print(rsa.shape)\n",
    "display(rsa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering\n",
    "Capture sessions attended per patient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patdeid</th>\n",
       "      <th>attendance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1915</th>\n",
       "      <td>1930</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1916</th>\n",
       "      <td>1931</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1917</th>\n",
       "      <td>1932</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1918</th>\n",
       "      <td>1933</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1919</th>\n",
       "      <td>1934</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1920 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      patdeid  attendance\n",
       "0           1          25\n",
       "1           2          25\n",
       "2           3          25\n",
       "3           4          25\n",
       "4           5           1\n",
       "...       ...         ...\n",
       "1915     1930           1\n",
       "1916     1931          25\n",
       "1917     1932           1\n",
       "1918     1933           1\n",
       "1919     1934           1\n",
       "\n",
       "[1920 rows x 2 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create df with count of attendance for each patient\n",
    "attendence = rsa.groupby('patdeid')['attendance'].size().to_frame('attendance').reset_index()\n",
    "\n",
    "attendence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patdeid</th>\n",
       "      <th>attendance_0</th>\n",
       "      <th>attendance_1</th>\n",
       "      <th>attendance_2</th>\n",
       "      <th>attendance_3</th>\n",
       "      <th>attendance_4</th>\n",
       "      <th>attendance_5</th>\n",
       "      <th>attendance_6</th>\n",
       "      <th>attendance_7</th>\n",
       "      <th>attendance_8</th>\n",
       "      <th>...</th>\n",
       "      <th>attendance_15</th>\n",
       "      <th>attendance_16</th>\n",
       "      <th>attendance_17</th>\n",
       "      <th>attendance_18</th>\n",
       "      <th>attendance_19</th>\n",
       "      <th>attendance_20</th>\n",
       "      <th>attendance_21</th>\n",
       "      <th>attendance_22</th>\n",
       "      <th>attendance_23</th>\n",
       "      <th>attendance_24</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1915</th>\n",
       "      <td>1930</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1916</th>\n",
       "      <td>1931</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1917</th>\n",
       "      <td>1932</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1918</th>\n",
       "      <td>1933</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1919</th>\n",
       "      <td>1934</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1920 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      patdeid  attendance_0  attendance_1  attendance_2  attendance_3  \\\n",
       "0           1           1.0           1.0           1.0           1.0   \n",
       "1           2           1.0           1.0           1.0           1.0   \n",
       "2           3           1.0           1.0           1.0           1.0   \n",
       "3           4           1.0           1.0           1.0           0.0   \n",
       "4           5           1.0           0.0           0.0           0.0   \n",
       "...       ...           ...           ...           ...           ...   \n",
       "1915     1930           1.0           0.0           0.0           0.0   \n",
       "1916     1931           1.0           1.0           1.0           1.0   \n",
       "1917     1932           1.0           0.0           0.0           0.0   \n",
       "1918     1933           1.0           0.0           0.0           0.0   \n",
       "1919     1934           1.0           0.0           0.0           0.0   \n",
       "\n",
       "      attendance_4  attendance_5  attendance_6  attendance_7  attendance_8  \\\n",
       "0              1.0           1.0           1.0           1.0           1.0   \n",
       "1              1.0           1.0           1.0           1.0           1.0   \n",
       "2              1.0           1.0           1.0           1.0           1.0   \n",
       "3              1.0           1.0           1.0           1.0           1.0   \n",
       "4              0.0           0.0           0.0           0.0           0.0   \n",
       "...            ...           ...           ...           ...           ...   \n",
       "1915           0.0           0.0           0.0           0.0           0.0   \n",
       "1916           1.0           1.0           1.0           1.0           1.0   \n",
       "1917           0.0           0.0           0.0           0.0           0.0   \n",
       "1918           0.0           0.0           0.0           0.0           0.0   \n",
       "1919           0.0           0.0           0.0           0.0           0.0   \n",
       "\n",
       "      ...  attendance_15  attendance_16  attendance_17  attendance_18  \\\n",
       "0     ...            1.0            0.0            1.0            1.0   \n",
       "1     ...            1.0            1.0            1.0            1.0   \n",
       "2     ...            1.0            1.0            1.0            1.0   \n",
       "3     ...            1.0            1.0            1.0            1.0   \n",
       "4     ...            0.0            0.0            0.0            0.0   \n",
       "...   ...            ...            ...            ...            ...   \n",
       "1915  ...            0.0            0.0            0.0            0.0   \n",
       "1916  ...            1.0            1.0            0.0            1.0   \n",
       "1917  ...            0.0            0.0            0.0            0.0   \n",
       "1918  ...            0.0            0.0            0.0            0.0   \n",
       "1919  ...            0.0            0.0            0.0            0.0   \n",
       "\n",
       "      attendance_19  attendance_20  attendance_21  attendance_22  \\\n",
       "0               1.0            1.0            1.0            1.0   \n",
       "1               1.0            1.0            1.0            1.0   \n",
       "2               1.0            1.0            1.0            1.0   \n",
       "3               1.0            1.0            1.0            1.0   \n",
       "4               0.0            0.0            0.0            0.0   \n",
       "...             ...            ...            ...            ...   \n",
       "1915            0.0            0.0            0.0            0.0   \n",
       "1916            1.0            1.0            1.0            1.0   \n",
       "1917            0.0            0.0            0.0            0.0   \n",
       "1918            0.0            0.0            0.0            0.0   \n",
       "1919            0.0            0.0            0.0            0.0   \n",
       "\n",
       "      attendance_23  attendance_24  \n",
       "0               1.0            1.0  \n",
       "1               1.0            1.0  \n",
       "2               1.0            1.0  \n",
       "3               0.0            1.0  \n",
       "4               0.0            0.0  \n",
       "...             ...            ...  \n",
       "1915            0.0            0.0  \n",
       "1916            0.0            1.0  \n",
       "1917            0.0            0.0  \n",
       "1918            0.0            0.0  \n",
       "1919            0.0            0.0  \n",
       "\n",
       "[1920 rows x 26 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set parameters to flatten the df\n",
    "start = 0 # include data starting from week 0\n",
    "end = 24 # finish at week 24\n",
    "step = 1 # include data for every week\n",
    "\n",
    "# call function to flatten dataframe\n",
    "rsa_flat = helper.flatten_dataframe(rsa, start, end, step)\n",
    "\n",
    "# fill nulls with 0 for no attendance\n",
    "rsa_flat = rsa_flat.fillna(0)\n",
    "\n",
    "# visually inspect the data\n",
    "rsa_flat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "We will create the feature for treatment dropout, an important metric.<br>\n",
    "Patients who do not show attendence for the final 4 weeks of treatment<br>\n",
    "they will be considered to have dropped out of treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dropout ratio is:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dropout\n",
       "1    0.620313\n",
       "0    0.379688\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create feature for treatment dropout\n",
    "# treatment dropout is defined when patient misses attendance for final 4 weeks of treatment\n",
    "rsa_flat['dropout'] = (\n",
    "                        rsa_flat \n",
    "                       .iloc[:,22:25] # look at the final 4 columns\n",
    "                       .sum(axis=1) # sum the values \n",
    "                       .apply(lambda x: 1 if x == 0 else 0) # if the sum is 0, then the patient dropped out\n",
    "                       )\n",
    "\n",
    "print('The dropout ratio is:')\n",
    "display(rsa_flat.dropout.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform Demographics Table\n",
    "This table came with inconsistent numbering convensions.<br>\n",
    "We will have to manually transform the values for each feature.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patdeid</th>\n",
       "      <th>dem_gender</th>\n",
       "      <th>dem_ethnicity</th>\n",
       "      <th>dem_race_no_answer</th>\n",
       "      <th>dem_race_unknown</th>\n",
       "      <th>dem_race_amer_ind</th>\n",
       "      <th>dem_race_asian</th>\n",
       "      <th>dem_race_black</th>\n",
       "      <th>dem_race_pacific_islander</th>\n",
       "      <th>dem_race_white</th>\n",
       "      <th>dem_race_other</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>male</td>\n",
       "      <td>not_spanish_origin</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>male</td>\n",
       "      <td>not_spanish_origin</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>not_spanish_origin</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>female</td>\n",
       "      <td>not_spanish_origin</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>male</td>\n",
       "      <td>not_spanish_origin</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1915</th>\n",
       "      <td>1930</td>\n",
       "      <td>female</td>\n",
       "      <td>not_spanish_origin</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1916</th>\n",
       "      <td>1931</td>\n",
       "      <td>male</td>\n",
       "      <td>not_spanish_origin</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1917</th>\n",
       "      <td>1932</td>\n",
       "      <td>female</td>\n",
       "      <td>not_spanish_origin</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1918</th>\n",
       "      <td>1933</td>\n",
       "      <td>male</td>\n",
       "      <td>not_spanish_origin</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1919</th>\n",
       "      <td>1934</td>\n",
       "      <td>male</td>\n",
       "      <td>not_spanish_origin</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1920 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      patdeid dem_gender       dem_ethnicity  dem_race_no_answer  \\\n",
       "0           1       male  not_spanish_origin                 0.0   \n",
       "1           2       male  not_spanish_origin                 0.0   \n",
       "2           3       male  not_spanish_origin                 0.0   \n",
       "3           4     female  not_spanish_origin                 0.0   \n",
       "4           5       male  not_spanish_origin                 0.0   \n",
       "...       ...        ...                 ...                 ...   \n",
       "1915     1930     female  not_spanish_origin                 0.0   \n",
       "1916     1931       male  not_spanish_origin                 0.0   \n",
       "1917     1932     female  not_spanish_origin                 0.0   \n",
       "1918     1933       male  not_spanish_origin                 0.0   \n",
       "1919     1934       male  not_spanish_origin                 0.0   \n",
       "\n",
       "      dem_race_unknown  dem_race_amer_ind  dem_race_asian  dem_race_black  \\\n",
       "0                  0.0                0.0             0.0             0.0   \n",
       "1                  0.0                0.0             0.0             0.0   \n",
       "2                  0.0                0.0             0.0             0.0   \n",
       "3                  0.0                0.0             0.0             0.0   \n",
       "4                  0.0                0.0             0.0             0.0   \n",
       "...                ...                ...             ...             ...   \n",
       "1915               0.0                0.0             0.0             0.0   \n",
       "1916               0.0                0.0             0.0             0.0   \n",
       "1917               0.0                0.0             0.0             0.0   \n",
       "1918               0.0                0.0             0.0             0.0   \n",
       "1919               0.0                0.0             0.0             0.0   \n",
       "\n",
       "      dem_race_pacific_islander  dem_race_white  dem_race_other  \n",
       "0                           0.0             1.0             0.0  \n",
       "1                           0.0             1.0             0.0  \n",
       "2                           0.0             1.0             0.0  \n",
       "3                           0.0             1.0             0.0  \n",
       "4                           0.0             1.0             0.0  \n",
       "...                         ...             ...             ...  \n",
       "1915                        0.0             1.0             0.0  \n",
       "1916                        0.0             1.0             0.0  \n",
       "1917                        0.0             1.0             0.0  \n",
       "1918                        0.0             1.0             0.0  \n",
       "1919                        0.0             1.0             0.0  \n",
       "\n",
       "[1920 rows x 11 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set parameters for transformation\n",
    "dem_cols = ['patdeid','DEM002','DEM003A','DEM004A','DEM004B','DEM004C','DEM004D','DEM004E',\n",
    "            'DEM004F','DEM004G','DEM004H']\n",
    "dem_labels = {'DEM002':'dem_gender','DEM003A':'dem_ethnicity','DEM004A':'dem_race_amer_ind',\n",
    "              'DEM004B':'dem_race_asian','DEM004C':'dem_race_black','DEM004D':'dem_race_pacific_islander',\n",
    "             'DEM004E':'dem_race_white','DEM004F':'dem_race_other','DEM004G':'dem_race_no_answer',\n",
    "                'DEM004H':'dem_race_unknown'}\n",
    "\n",
    "# the helper function will clean and transform the data\n",
    "dem = helper.clean_df(dem, dem_cols, dem_labels)\n",
    "\n",
    "# for ethnicity column, map 1:'spanish_origin', 2:'not_spanish_origin', to values\n",
    "for col in dem.columns:\n",
    "    if col =='dem_ethnicity':\n",
    "        dem[col] = dem[col].replace({1:'spanish_origin',2:'not_spanish_origin'})\n",
    "    if col =='dem_gender':\n",
    "        dem[col] = dem[col].replace({1:'male',2:'female'})\n",
    "    if col =='dem_race_asian':\n",
    "        dem[col] = dem[col].replace({2:1})\n",
    "    if col=='dem_race_black':\n",
    "        dem[col] = dem[col].replace({3:1})\n",
    "    if col=='dem_race_pacific_islander':\n",
    "        dem[col] = dem[col].replace({4:1})\n",
    "    if col=='dem_race_white':\n",
    "        dem[col] = dem[col].replace({5:1})\n",
    "    if col=='dem_race_other':\n",
    "        dem[col] = dem[col].replace({6:1})\n",
    "    if col=='dem_race_no_answer':\n",
    "        dem[col] = dem[col].replace({7:1})\n",
    "    if col=='dem_race_unknown':\n",
    "        dem[col] = dem[col].replace({8:1})\n",
    "\n",
    "# imputation strategy: 0 for missing values, purpose is for counts of dem data\n",
    "dem = dem.fillna(0)\n",
    "\n",
    "# review the data\n",
    "dem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform Urine Drug Screen Table\n",
    "This table contains the data for most of the outcome metrics<br>\n",
    "Stay tuned for feature engineering section towards the end of this table transformation<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set parameters for transformation\n",
    "uds_cols = ['patdeid','VISIT', 'UDS005', 'UDS006', 'UDS007', 'UDS008', 'UDS009', 'UDS010', 'UDS011', 'UDS012', \n",
    "            'UDS013']\n",
    "uds_labels = {'UDS005':'test_Amphetamines', 'UDS006':'test_Benzodiazepines','UDS007':'test_Methadone', \n",
    "              'UDS008':'test_Oxycodone', 'UDS009':'test_Cocaine', 'UDS010':'test_Methamphetamine', 'UDS011':'test_Opiate300', 'UDS012':'test_Cannabinoids', 'UDS013':'test_Propoxyphene'}\n",
    "\n",
    "# the helper function will clean and transform the data\n",
    "uds = helper.clean_df(uds, uds_cols, uds_labels)\n",
    "\n",
    "print('Dataframe uds with shape of', uds.shape, 'has been cleaned')\n",
    "display(uds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe is ready to be flattened\n",
    "\n",
    "# set params for flattening\n",
    "start = 0 # include data starting from week 0\n",
    "end = 24 # finish at week 24\n",
    "step = 1 # include data for every week\n",
    "\n",
    "# call function to flatten dataframe\n",
    "uds_flat = helper.flatten_dataframe(uds, start, end, step)\n",
    "\n",
    "# fill missing values with 1, which is a binary value for positive test\n",
    "uds_flat.fillna(1, inplace=True)\n",
    "\n",
    "# visually inspect the data\n",
    "print('The clinical data was added in the form of',uds_flat.shape[1],'features')\n",
    "print('Which includes tests for 8 different drug classes over 24 weeks')\n",
    "display(uds_flat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering\n",
    "The following metrics will be created to assess treatment success:<br>\n",
    "- `TNT` - (numeric) - total negative tests, a measure of clinical benefit, count of negative tests over 24 weeks\n",
    "- `NTR` - (float) - negative test rate, a measure of clinical benefit, percentage of negative tests over 24 weeks\n",
    "- `CNT` - (numeric) - concsecutive negative tests, a measure of clinical benefit, count of consecutive negative tests over 24 weeks\n",
    "- `responder` - (binary) - indicating if the patient responds to treatment, by testing negative for opiates for the final 4 weeks of treatment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call the helper function to create the UDS features\n",
    "uds_features = helper.uds_features(uds_flat)\n",
    "\n",
    "# isolate the features df for merge with the clinical data\n",
    "uds_features = uds_features[['patdeid','TNT','NTR','CNT','responder']]\n",
    "\n",
    "print('The UDS features have been created with shape of', uds_features.shape)\n",
    "display(uds_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform DSM-IV Diagnosis Table\n",
    "The values for these features are mapped as follows:<br>\n",
    "<br>\n",
    "1 = Dependence<br>\n",
    "2 = Abuse<br>\n",
    "3 = No Diagnosis<br>\n",
    "<br>\n",
    "This will require one hot encoding in the datapipelines later on.<br>\n",
    "We will label the values as text strings, so that they can appear<br>\n",
    "on columns in the final dataset. The text strings will also help<br>\n",
    "with analysis<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set params for transformation\n",
    "dsm_cols = ['patdeid','DSMOPI','DSMAL','DSMAM','DSMCA','DSMCO','DSMSE']\n",
    "dsm_labels = {'DSMOPI':'dsm_opiates','DSMAL':'dsm_alcohol','DSMAM':'dsm_amphetamine',\n",
    "              'DSMCA':'dsm_cannabis','DSMCO':'dsm_cocaine','DSMSE':'dsm_sedative'}\n",
    "\n",
    "# call the helper function to clean the data\n",
    "dsm = helper.clean_df(dsm, dsm_cols, dsm_labels)\n",
    "\n",
    "# convert cols to numeric\n",
    "dsm = dsm.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# convert values to text strings as follows:\n",
    "# 1 - dependence, 2 - abuse, 3 - no diagnosis, 0 - not present\n",
    "dsm = dsm.replace({1:'dependence', 2:'abuse', 3:'no_diagnosis'})\n",
    "\n",
    "# fill nulls with 0, where patient does not confirm diagnosis\n",
    "dsm.fillna('not_present', inplace=True)\n",
    "\n",
    "print('Dataframe dsm with shape of', dsm.shape, 'has been cleaned')\n",
    "display(dsm[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform Medical and Psychiatric History Table\n",
    "We will track 18 different medical conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set parameters for transformation\n",
    "mdh_cols = ['patdeid','MDH001','MDH002','MDH003','MDH004','MDH005','MDH006','MDH007','MDH008','MDH009',\n",
    "            'MDH010','MDH011A','MDH011B','MDH012','MDH013','MDH014','MDH015','MDH016','MDH017']\n",
    "mdh_labels = {'MDH001':'mdh_head_injury','MDH002':'mdh_allergies','MDH003':'mdh_liver_problems',\n",
    "                'MDH004':'mdh_kidney_problems','MDH005':'mdh_gi_problems','MDH006':'mdh_thyroid_problems',\n",
    "                'MDH007':'mdh_heart_condition','MDH008':'mdh_asthma','MDH009':'mdh_hypertension',\n",
    "                'MDH010':'mdh_skin_disease','MDH011A':'mdh_opi_withdrawal','MDH011B':'mdh_alc_withdrawal',\n",
    "                'MDH012':'mdh_schizophrenia','MDH013':'mdh_major_depressive_disorder',\n",
    "                'MDH014':'mdh_bipolar_disorder','MDH015':'mdh_anxiety_disorder','MDH016':'mdh_sig_neurological_damage','MDH017':'mdh_epilepsy'}\n",
    "\n",
    "# call the helper function to clean the data\n",
    "mdh = helper.clean_df(mdh, mdh_cols, mdh_labels)\n",
    "\n",
    "# map values to txt strings, 0 = no_history, 1 = yes_history, 9 = not_evaluated, skip the first column\n",
    "for col in mdh.columns[1:]:\n",
    "    mdh[col] = mdh[col].map({0:'no_history', 1:'yes_history', 9:'not_evaluated'})\n",
    "\n",
    "# fill in the nulls, but skip the patdeid column\n",
    "mdh = mdh.fillna('not_evaluated')\n",
    "\n",
    "\n",
    "# visually inspect the data\n",
    "print('Dataframe mdh with shape of', mdh.shape, 'has been cleaned')\n",
    "display(mdh[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform the PEX (Physical Exam) Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set params to clean cols\n",
    "pex_cols = ['patdeid','PEX001A','PEX002A','PEX003A','PEX004A','PEX005A','PEX006A','PEX007A',\n",
    "            'PEX008A','PEX009A','PEX010A','PEX011A','PEX012A','VISIT']\n",
    "pex_labels = {'PEX001A':'pex_gen_appearance','PEX002A':'pex_head_neck','PEX003A':'pex_ears_nose_throat',\n",
    "              'PEX004A':'pex_cardio','PEX005A':'pex_lymph_nodes','PEX006A':'pex_respiratory',\n",
    "              'PEX007A':'pex_musculoskeletal','PEX008A':'pex_gi_system','PEX009A':'pex_extremeties',\n",
    "              'PEX010A':'pex_neurological','PEX011A':'pex_skin','PEX012A':'pex_other'}\n",
    "\n",
    "# this dataset includes data from visit BASELINE and 24, we are only interested in BASELINE\n",
    "pex = pex.loc[pex.VISIT=='BASELINE']\n",
    "              \n",
    "# call the helper function to clean the data\n",
    "pex = helper.clean_df(pex, pex_cols, pex_labels)\n",
    "\n",
    "# map values to strings, 0 = normal, 1 = abnormal, 9 = not_evaluated\n",
    "for col in pex.columns[2:]:\n",
    "    pex[col] = pex[col].map({0:'normal', 1:'abnormal', 9:'not_evaluated'})\n",
    "\n",
    "# imputation strategy: 9 indicates no diagnosis\n",
    "pex.fillna('not_evaluated', inplace=True)\n",
    "\n",
    "# drop the visit column\n",
    "pex.drop(columns='VISIT', inplace=True)\n",
    "\n",
    "# visually inspect the data\n",
    "print('Dataframe pex with shape of', pex.shape, 'has been cleaned')\n",
    "display(pex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform the Pregnancy and Birth Control Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define parameters for cleaning\n",
    "pbc_cols = ['patdeid','VISIT','PBC003']\n",
    "pbc_labels = {'PBC003':'pbc_test_result'} \n",
    "\n",
    "# call the helper function to clean the data\n",
    "pbc = helper.clean_df(pbc, pbc_cols, pbc_labels)\n",
    "\n",
    "# remove followup visits from the main clinical after week 24\n",
    "pbc = pbc[~pbc['VISIT'].isin([28, 32])]\n",
    "\n",
    "pbc = pbc.fillna(0)\n",
    "\n",
    "pbc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten the pbc data\n",
    "\n",
    "# set parameters for flattening\n",
    "start = 0 # include data starting from week 0\n",
    "end = 24 # finish at week 24\n",
    "step = 4 # include data for every week\n",
    "\n",
    "pbc_flat = helper.flatten_dataframe(pbc, start, end, step)\n",
    "\n",
    "pbc_flat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform the TFB (Timeline Follow Back Survey) Table\n",
    "- This table has an issue with multiple rows per patient\n",
    "- Each report of drug use is recorded in a new row\n",
    "- We will aggregate the data to a single row per patient\n",
    "- After the aggregation, the table will be flattened, to encode the survey, drug class and week collected, in each column\n",
    "- Surveys are collected once a month and reflect the previous 30 days of drug use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define parameters for cleaning\n",
    "tfb_cols = ['patdeid','VISIT','TFB001A','TFB002A','TFB003A','TFB004A','TFB005A','TFB006A','TFB007A',\n",
    "            'TFB008A','TFB009A','TFB010A']\n",
    "tfb_labels = {'TFB001A':'survey_alcohol','TFB002A':'survey_cannabis','TFB003A':'survey_cocaine',    \n",
    "              'TFB010A':'survey_oxycodone','TFB009A':'survey_methadone','TFB004A':'survey_amphetamine','TFB005A':'survey_methamphetamine','TFB006A':'survey_opiates','TFB007A':'survey_benzodiazepines','TFB008A':'survey_propoxyphene'}\n",
    "\n",
    "# call the helper function to clean the data\n",
    "tfb = helper.clean_df(tfb, tfb_cols, tfb_labels)\n",
    "\n",
    "# visually inspect the data\n",
    "print('Shape of cleaned tfb dataframe is', tfb.shape)\n",
    "display(tfb[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregate rows by patient and visit, sum all records of drug use\n",
    "\n",
    "# create index\n",
    "index = ['patdeid','VISIT']\n",
    "\n",
    "# create aggregation dictionary, omit the first two columns, they do not require aggregation\n",
    "agg_dict = {col:'sum' for col in tfb.columns[2:]}\n",
    "\n",
    "# aggregate the data, we will apply sum to all instances of reported us to give the total use for the period\n",
    "tfb_agg = tfb.groupby(index).agg(agg_dict).reset_index()\n",
    "\n",
    "# visually inspect the data\n",
    "print('Aggregated tfb dataframe contains', tfb_agg.shape[0],'rows, coming from', tfb.shape[0],'rows')\n",
    "display(tfb_agg[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten the dataframe\n",
    "\n",
    "# set parameters to flatten survey data\n",
    "start = 0 # include data starting from week 0\n",
    "end = 24 # finish at week 24\n",
    "step = 4 # include data for every 4 weeks\n",
    "\n",
    "# call function to flatten dataframe\n",
    "tfb_flat = helper.flatten_dataframe(tfb_agg, start, end, step)\n",
    "\n",
    "# imputation strategy: fill missing values with 0, indicates no drug use\n",
    "tfb_flat.fillna(0, inplace=True)\n",
    "\n",
    "# visualize the data\n",
    "print('Flattended dataframe contains', tfb_flat.shape[1]-1,'features')\n",
    "display(tfb_flat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform the Medication Dose Table\n",
    "- This table has an issue with multiple rows per patient\n",
    "- Each dose of medication is recorded as a row\n",
    "- This means that if a patient received 7 doses of medication, there will be 7 rows for that patient\n",
    "- This needs to be consolidated into a single row per patient\n",
    "- For total_dose with null values, we will treat that as a no show or 0 dose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set parameters for cleaning the dataframe\n",
    "dos_cols = ['patdeid','VISIT','DOS002','DOS005']   \n",
    "dos_labels = {'DOS002':'medication','DOS005':'total_dose'}\n",
    "\n",
    "# call the helper function to clean the data\n",
    "dos = helper.clean_df(dos, dos_cols, dos_labels)\n",
    "\n",
    "# Imputation strategy: backfill and forwardfill missing values from medication and total dose\n",
    "dos['medication'] = dos['medication'].fillna(method='ffill').fillna(method='bfill')\n",
    "dos['total_dose'] = dos['total_dose'].fillna(method='ffill').fillna(method='bfill')\n",
    "\n",
    "# observe the data\n",
    "print('The medication dataframe contains', dos.shape[0],'rows that must be aggregated')\n",
    "display(dos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregate columns \n",
    "\n",
    "# create index\n",
    "index = ['patdeid','VISIT','medication']\n",
    "# create aggregation dictionary\n",
    "agg_dict = {col:'sum' for col in dos.columns[3:]}\n",
    "\n",
    "# aggregate the data, we will add daily dose to create weekly dose total, aggregating multiple columns per patient\n",
    "dos_agg = dos.groupby(index).agg(agg_dict).reset_index()\n",
    "\n",
    "# create df with patdeid and medication to merge later, this will help make analysis easier\n",
    "medication = dos[['patdeid', 'medication']].drop_duplicates(subset=['patdeid'], keep='first').reset_index(drop=True)\n",
    "\n",
    "# visualize the data\n",
    "print('Total rows in the aggregated dataframe:', dos_agg.shape[0],'from', dos.shape[0],'rows')\n",
    "dos_agg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering\n",
    "Create separate columns for bupe and methadone, this improves data quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature engineering\n",
    "\n",
    "# call helper function to create features from the medication data\n",
    "dos_agg = helper.med_features(dos_agg)\n",
    "\n",
    "# visually inspect the data\n",
    "print('The aggregated dataframe contains', dos_agg.shape[1]-2,'features')\n",
    "display(dos_agg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten the dataframe\n",
    "\n",
    "# set parameters to flatten the dataframe\n",
    "start = 0 # include data starting from week 0\n",
    "end = 24 # finish at week 24\n",
    "step = 1 # include data for every week\n",
    "\n",
    "# call function to flatten dataframe\n",
    "dos_flat = helper.flatten_dataframe(dos_agg, start, end, step)\n",
    "\n",
    "# imputation strategy: nulls come post merge, these were visits for patients who dropped out, fill with 0\n",
    "dos_flat.fillna(0, inplace=True)\n",
    "\n",
    "print('The flattened dataframe contains', dos_flat.shape[1]-1,'features')\n",
    "display(dos_flat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we will merge all the tables into a single dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set parameters for merge\n",
    "\n",
    "# Define the dataframes to merge\n",
    "dfs = [rsa_flat, dos_flat, uds_flat, tfb_flat, \n",
    "       pbc_flat, uds_features, dem, dsm, mdh, \n",
    "       pex, medication, attendence]\n",
    "\n",
    "# Initialize merged_df with the first DataFrame in the list\n",
    "merged_df = dfs[0]\n",
    "\n",
    "# Merge the dfs above using left merge on 'patdeid'\n",
    "for df in dfs[1:]:  # Start from the second item in the list\n",
    "    merged_df = pd.merge(merged_df, df, on='patdeid', how='left')\n",
    "\n",
    "# some rows were duplicated from one:many merge, they will be dropped\n",
    "merged_df = merged_df.drop_duplicates(subset=['patdeid'], keep='first')\n",
    "\n",
    "# Print the shape of the final dataframe\n",
    "print('The final table includes', merged_df.shape[1]-1, 'features for', merged_df.shape[0], 'patients in treatment')\n",
    "\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze Null Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show all rows from the function call\n",
    "pd.set_option('display.max_rows', None)\n",
    "merged_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputation Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the medication column we will backfill and forward fill the nulls\n",
    "# these patients dropped out, however, we would like to closely track their meds where possible\n",
    "merged_df.medication = merged_df.medication.fillna(method='ffill').fillna(method='bfill')\n",
    "\n",
    "# for columns that have 'meds' in the column name, forwardfill and backfill nulls\n",
    "# these columns are the daily dose of medication\n",
    "for col in merged_df.columns:\n",
    "    if 'meds' in col:\n",
    "        merged_df[col] = merged_df[col].fillna(method='ffill').fillna(method='bfill')\n",
    "\n",
    "# for the sae and pbc columns, the nulls are just patients without data, can be set to 0\n",
    "# for the pex columns, nulls come from patients who dropped from treatment\n",
    "# can be filled with 0\n",
    "\n",
    "# create list with prefix of columns to fill for zero value\n",
    "cols1 = ['survey','pbc']\n",
    "for col in merged_df.columns:\n",
    "    if any(x in col for x in cols1):\n",
    "        merged_df[col] = merged_df[col].fillna(0)\n",
    "\n",
    "# set nulls in mdh to not_evaluated\n",
    "cols2 = ['pex', 'dsm', 'mdh']\n",
    "for col in merged_df.columns:\n",
    "    if any(x in col for x in cols2):\n",
    "        merged_df[col] = merged_df[col].fillna('not_evaluated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete rows for incomplete patient profiles -  334, 1003 and 1006\n",
    "merged_df = merged_df[~merged_df['patdeid'].isin([334, 1003, 1006])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to data folder in csv\n",
    "merged_df.to_csv('../data/merged_data.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
