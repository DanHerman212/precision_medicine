{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning Strategy\n",
    "For this project, we will use the CTN0027 Dataset from the clinical trial network.<br>\n",
    "The dataset and all it's documentation are avaialble at the following website:<br>\n",
    "https://datashare.nida.nih.gov/study/nida-ctn-0027<br>\n",
    "\n",
    "There are a few challenges to highlight that should be considered in the cleaning approach:\n",
    "- Large de-identified dataset must be manually labeled, very time consuming and prone to errors\n",
    "- High dimension data - Requires bespoke transformations to fit into machine learning models\n",
    "\n",
    "## Tables to be cleaned\n",
    "| File Name | Table Name | Variable |Description | Process Applied |\n",
    "| :--- | :--- | :--- | :--- | :--- |\n",
    "| T_FRRSA.csv | Research Session Attendance|RSA |Records attendence for each week of treatment | Clean, Flatten, Feature Extraction, Merge |\n",
    "| T_FRDEM.csv | Demographics|DEM |Sex, Ethnicity, Race | Clean, Merge |\n",
    "| T_FRUDSAB.csv | Urine Drug Screen| UDS  |Drug test for 8 different drug classes, taken weekly for 24 weeks | Clean, Flatten, Feature Extraction, Merge |\n",
    "| T_FRDSM.csv | DSM-IV Diagnosis|DSM |Tracks clinical diagnosis for substance use disorder, in accordance with DSM guidelines| Clean, Merge |\n",
    "| T_FRMDH.csv | Medical and Psychiatric History|MDH |Tracks medical and psychiatric history of 24 different Conditions| Clean, Merge |\n",
    "| T_FRPEX.csv | Physical Exam|PEX |Tracks the appearance and condition of patients for 12 different physical observations| Clean, Merge |\n",
    "| T_FRTFB.csv | Timeline Follow Back Survey|TFB |Surveys for self reported drug use, collected every 4 weeks, includes previous 30 days of use ot week 0, 4, 8, 12, 16, 20, 24| Clean, Aggregate, Flatten, Merge |\n",
    "|T_FRDOS.csv | Dose Record |DOS |Records the medication, averge weekly dose and week of treatment| Clean, Aggregate, Feature Extraction, Flatten, Merge |\n",
    "|T_FRCOWS.csv| Clinical Opiate Withdrawal Scale (Predose)|CW1 |Records the severity of opiate withdrawal symptoms, taken at assessment| Clean, Merge |\n",
    "|T_FRCOWS2.csv| Clinical Opiate Withdrawal Scale (Postdose)|CW2 |Records the severity of opiate withdrawal symptoms, taken after dose| Clean, Merge |\n",
    "\n",
    "\n",
    "## Data Cleaning Process\n",
    "We will try to keep things simple and employ a process driven by reusable functions<br>\n",
    "to **improve data quality**, **reduce time to market** and **reducing human error**.<br>\n",
    "<br> \n",
    "For each table we will follow the following steps:<br>\n",
    "1. Load the data\n",
    "2. Identify columns that require labels\n",
    "3. Apply labels to columns\n",
    "4. Drop columns that are not needed\n",
    "5. Create imputation strategy for missing values\n",
    "5. Apply transformations to values where required\n",
    "3. Feature Engineering (if necessary)\n",
    "4. Flatten Dataframes (encode week of treatment into columns, where applicable)\n",
    "4. Merge with other tables\n",
    "\n",
    "## List of Reusable Functions\n",
    "| Name of Function | Description | \n",
    "| ---------------- | ----------- |\n",
    "| clean_df | Clean the given DataFrame by dropping unnecessary columns, renaming columns, and reordering columns. |\n",
    "| flatten_dataframe | This function creates features by combining the VISIT column with the clinical datapoint (see example below).  The goal is to reduce individual rows per patient.  The data currently presents 25 rows per patient (for each week of treatment), which won't work for machine learning.  The model will only accept one row per patient, so we must encode all the clinical data into columns.  We will tranform the data by creating a separate dataframe for each week of treatment.  We will encode the week of treatment into the columns in each dataframe and then merge them together to form a high quality dataset, with granular level treatment data, that should help improve machine learning model accuracy.  This is a complex transformation, but justified for the incremental improvement to machine learning accuracy |\n",
    "| merge_dfs | Merge the given list of DataFrames into one DataFrame. |\n",
    "| uds_features | Creates 4 new features which are metrics used to measure outcomes from opiate test data. |\n",
    "| med_features | Creates 2 new features for medication dose to enrich dataset and improve accuracy in machine learning|\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # data manipulation library\n",
    "import numpy as np # numerical computing library\n",
    "import matplotlib.pyplot as plt # data visualization library\n",
    "import seaborn as sns # advanced data visualization library\n",
    "import helper # custom fuctions I created to clean and plot data\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Data\n",
    "We will load 10 files from the de-identified dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define parameters to load data\n",
    "\n",
    "# define the path to the data\n",
    "data_path = '../unlabeled_data/'\n",
    "\n",
    "# define the names of the files to load\n",
    "file_names = ['T_FRRSA.csv', 'T_FRUDSAB.csv', 'T_FRDSM.csv',\n",
    "              'T_FRMDH.csv', 'T_FRPEX.csv',   'T_FRTFB.csv',\n",
    "              'T_FRDOS.csv', 'T_FRCOWS.csv',  'T_FRCOWS2.csv',\n",
    "              'T_FRRBS0A.csv', 'T_FRDEM.csv']\n",
    "\n",
    "# define the names of the variables for the dataframes\n",
    "variables = ['rsa', 'uds', 'dsm', 'mdh', 'pex', \n",
    "             'tfb', 'dos', 'cw1', 'cw2', 'rbs','dem']\n",
    "\n",
    "# create a loop to iterate through the files and load them into the notebook\n",
    "for file_name, variable in zip(file_names, variables):\n",
    "        globals()[variable] = pd.read_csv(data_path + file_name)\n",
    "        print(f\"{variable} shape: {globals()[variable].shape}\") # print the shape of the dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform Attendence Table\n",
    "- This table establishes the patient population and will serve as the primary table\n",
    "- All subsequent tables will use a LEFT JOIN to add clinical data as columns to each patient ID\n",
    "- This table requires feature engineering for `attendance` and `dropout` variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will define the columns and labels that we need for each df and then transform the data\n",
    "\n",
    "# set parameters for transformation\n",
    "rsa_cols = ['patdeid','VISIT','RSA001']\n",
    "rsa_labels = {'RSA001':'rsa_week'}\n",
    "\n",
    "# the helper function will transform the data\n",
    "rsa = helper.clean_df(rsa, rsa_cols, rsa_labels)\n",
    "\n",
    "# remove the followup visits from the main clinical data weeks 0 - 24\n",
    "# rsa = rsa[~rsa['VISIT'].isin([28, 32])]\n",
    "\n",
    "# remove duplicate rows\n",
    "rsa = rsa.drop_duplicates(subset=['patdeid', 'VISIT'], keep='first')\n",
    "\n",
    "# observe shape and sample 5 observations\n",
    "print(rsa.shape)\n",
    "display(rsa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering\n",
    "Capture weeks completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create df with count of attendance for each patient\n",
    "attendence = rsa.groupby('patdeid')['rsa_week'].size().to_frame('weeks_attended').reset_index()\n",
    "\n",
    "attendence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set parameters to flatten the df\n",
    "start = 0 # include data starting from week 0\n",
    "end = 32 # finish at week 24\n",
    "step = 1 # include data for every week\n",
    "\n",
    "# call function to flatten dataframe\n",
    "rsa_flat = helper.flatten_dataframe(rsa, start, end, step)\n",
    "\n",
    "# fill nulls with 0 for no attendance\n",
    "rsa_flat = rsa_flat.fillna(0)\n",
    "\n",
    "# there are follow up visits for week 28 and 32\n",
    "# the flatten_dataframes() function will complete a sequence\n",
    "# there are erroneous columns created in the sequence\n",
    "# remove columns rsa_week_25, rsa_week_26, rsa_week_27, rsa_week_29, rsa_week_30, rsa_week_31\n",
    "columns_to_drop = ['rsa_week_25', 'rsa_week_26', 'rsa_week_27', 'rsa_week_29', 'rsa_week_30', 'rsa_week_31']\n",
    "rsa_flat = rsa_flat.drop(columns=columns_to_drop)\n",
    "\n",
    "# visually inspect the data\n",
    "rsa_flat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering\n",
    "- Week 32 is a followup week to measure paitent dropout.\n",
    "- If patients do not attend week 32, they are considered to have dropped out of the program\n",
    "- We will rename rsa_week_32 to `dropout` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename rsa_week_28 to dropout\n",
    "rsa_flat = rsa_flat.rename(columns={'rsa_week_32':'dropout'})\n",
    "\n",
    "# reverse the values, 0 to 1, reflecting no attendence indicates dropout\n",
    "rsa_flat['dropout'] = rsa_flat['dropout'].replace({0:1, 1:0})\n",
    "\n",
    "rsa_flat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform Urine Drug Screen Table\n",
    "This table contains the data for most of the outcome metrics<br>\n",
    "Stay tuned for feature engineering section towards the end of this table transformation<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set parameters for transformation\n",
    "uds_cols = ['patdeid','VISIT', 'UDS005', 'UDS006', 'UDS007', 'UDS008', 'UDS009', 'UDS010', 'UDS011', 'UDS012', \n",
    "            'UDS013']\n",
    "uds_labels = {'UDS005':'test_Amphetamines', 'UDS006':'test_Benzodiazepines','UDS007':'test_MMethadone', \n",
    "              'UDS008':'test_Oxycodone', 'UDS009':'test_Cocaine', 'UDS010':'test_Methamphetamine', 'UDS011':'test_Opiate300', 'UDS012':'test_Cannabinoids', 'UDS013':'test_Propoxyphene'}\n",
    "\n",
    "# the helper function will clean and transform the data\n",
    "uds = helper.clean_df(uds, uds_cols, uds_labels)\n",
    "\n",
    "# for values -5 which indicate unclear, replace with 1 indicating positive\n",
    "uds = uds.replace(-5.0, 0)\n",
    "\n",
    "print('Dataframe uds with shape of', uds.shape, 'has been cleaned')\n",
    "display(uds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe is ready to be flattened\n",
    "\n",
    "# set params for flattening\n",
    "start = 0 # include data starting from week 0\n",
    "end = 24 # finish at week 24\n",
    "step = 1 # include data for every week\n",
    "\n",
    "# call function to flatten dataframe\n",
    "uds_flat = helper.flatten_dataframe(uds, start, end, step)\n",
    "\n",
    "# fill missing values with 1, which is a binary value for positive test\n",
    "uds_flat.fillna(1, inplace=True)\n",
    "\n",
    "# visually inspect the data\n",
    "print('The clinical data was added in the form of',uds_flat.shape[1],'features')\n",
    "print('Which includes tests for 8 different drug classes over 24 weeks')\n",
    "display(uds_flat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering\n",
    "The following metrics will be created to assess treatment success:<br>\n",
    "- `TNT` - (numeric) - total negative tests, a measure of clinical benefit, count of negative tests over 24 weeks\n",
    "- `NTR` - (float) - negative test rate, a measure of clinical benefit, percentage of negative tests over 24 weeks\n",
    "- `CNT` - (numeric) - concsecutive negative tests, a measure of clinical benefit, count of consecutive negative tests over 24 weeks\n",
    "- `responder` - (binary) - indicating if the patient responds to treatment, by testing negative for opiates for the final 4 weeks of treatment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call the helper function to create the UDS features\n",
    "uds_features = helper.uds_features(uds_flat)\n",
    "\n",
    "# isolate the features df for merge with the clinical data\n",
    "uds_features = uds_features[['patdeid','TNT','NTR','CNT','responder']]\n",
    "\n",
    "print('The UDS features have been created with shape of', uds_features.shape)\n",
    "display(uds_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform DSM-IV Diagnosis Table\n",
    "The values for these features are mapped as follows:<br>\n",
    "<br>\n",
    "1 = Dependence<br>\n",
    "2 = Abuse<br>\n",
    "3 = No Diagnosis<br>\n",
    "<br>\n",
    "This will require one hot encoding in the datapipelines later on.<br>\n",
    "We will label the values as text strings, so that they can appear<br>\n",
    "on columns in the final dataset. The text strings will also help<br>\n",
    "with analysis<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set params for transformation\n",
    "dsm_cols = ['patdeid','DSMOPI','DSMAL','DSMAM','DSMCA','DSMCO','DSMSE']\n",
    "dsm_labels = {'DSMOPI':'dsm_opiates','DSMAL':'dsm_alcohol','DSMAM':'dsm_amphetamine',\n",
    "              'DSMCA':'dsm_cannabis','DSMCO':'dsm_cocaine','DSMSE':'dsm_sedative'}\n",
    "\n",
    "# call the helper function to clean the data\n",
    "dsm = helper.clean_df(dsm, dsm_cols, dsm_labels)\n",
    "\n",
    "# convert cols to numeric\n",
    "dsm = dsm.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# convert values to text strings as follows after the first column\n",
    "# 1 - dependence, 2 - abuse, 3 - no diagnosis, 0 - not present\n",
    "for col in dsm.columns[1:]:\n",
    "    dsm[col] = dsm[col].replace({1:'dependence',2:'abuse',3:'no_diagnosis',0:'not_present'})\n",
    "\n",
    "\n",
    "# fill nulls with 0, where patient does not confirm diagnosis\n",
    "dsm.fillna('not_present', inplace=True)\n",
    "\n",
    "print('Dataframe dsm with shape of', dsm.shape, 'has been cleaned')\n",
    "display(dsm[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform Medical and Psychiatric History Table\n",
    "We will track 18 different medical conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set parameters for transformation\n",
    "mdh_cols = ['patdeid','MDH001','MDH002','MDH003','MDH004','MDH005','MDH006','MDH007','MDH008','MDH009',\n",
    "            'MDH010','MDH011A','MDH011B','MDH012','MDH013','MDH014','MDH015','MDH016','MDH017']\n",
    "mdh_labels = {'MDH001':'mdh_head_injury','MDH002':'mdh_allergies','MDH003':'mdh_liver_problems',\n",
    "                'MDH004':'mdh_kidney_problems','MDH005':'mdh_gi_problems','MDH006':'mdh_thyroid_problems',\n",
    "                'MDH007':'mdh_heart_condition','MDH008':'mdh_asthma','MDH009':'mdh_hypertension',\n",
    "                'MDH010':'mdh_skin_disease','MDH011A':'mdh_opi_withdrawal','MDH011B':'mdh_alc_withdrawal',\n",
    "                'MDH012':'mdh_schizophrenia','MDH013':'mdh_major_depressive_disorder',\n",
    "                'MDH014':'mdh_bipolar_disorder','MDH015':'mdh_anxiety_disorder','MDH016':'mdh_sig_neurological_damage','MDH017':'mdh_epilepsy'}\n",
    "\n",
    "# call the helper function to clean the data\n",
    "mdh = helper.clean_df(mdh, mdh_cols, mdh_labels)\n",
    "\n",
    "# map values to txt strings, 0 = no_history, 1 = yes_history, 9 = not_evaluated, skip the first column\n",
    "for col in mdh.columns[1:]:\n",
    "    mdh[col] = mdh[col].map({0:'no_history', 1:'yes_history', 9:'not_evaluated'})\n",
    "\n",
    "# fill in the nulls, but skip the patdeid column\n",
    "mdh = mdh.fillna('not_evaluated')\n",
    "\n",
    "# visually inspect the data\n",
    "print('Dataframe mdh with shape of', mdh.shape, 'has been cleaned')\n",
    "display(mdh[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform the PEX (Physical Exam) Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set params to clean cols\n",
    "pex_cols = ['patdeid','PEX001A','PEX002A','PEX003A','PEX004A','PEX005A','PEX006A','PEX007A',\n",
    "            'PEX008A','PEX009A','PEX010A','PEX011A','PEX012A','VISIT']\n",
    "pex_labels = {'PEX001A':'pex_gen_appearance','PEX002A':'pex_head_neck','PEX003A':'pex_ears_nose_throat',\n",
    "              'PEX004A':'pex_cardio','PEX005A':'pex_lymph_nodes','PEX006A':'pex_respiratory',\n",
    "              'PEX007A':'pex_musculoskeletal','PEX008A':'pex_gi_system','PEX009A':'pex_extremeties',\n",
    "              'PEX010A':'pex_neurological','PEX011A':'pex_skin','PEX012A':'pex_other'}\n",
    "\n",
    "# this dataset includes data from visit BASELINE and 24, we are only interested in BASELINE\n",
    "pex = pex.loc[pex.VISIT=='BASELINE']\n",
    "              \n",
    "# call the helper function to clean the data\n",
    "pex = helper.clean_df(pex, pex_cols, pex_labels)\n",
    "\n",
    "# map values to strings, 0 = normal, 1 = abnormal, 9 = not_evaluated\n",
    "for col in pex.columns[2:]:\n",
    "    pex[col] = pex[col].map({1:'normal', 2:'abnormal', 9:'not_evaluated'})\n",
    "\n",
    "# imputation strategy: 9 indicates no diagnosis\n",
    "pex.fillna('not_present', inplace=True)\n",
    "\n",
    "# drop the visit column\n",
    "pex.drop(columns='VISIT', inplace=True)\n",
    "\n",
    "# visually inspect the data\n",
    "print('Dataframe pex with shape of', pex.shape, 'has been cleaned')\n",
    "display(pex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform the TFB (Timeline Follow Back Survey) Table\n",
    "- This table has an issue with multiple rows per patient\n",
    "- Each report of drug use is recorded in a new row\n",
    "- We will aggregate the data to a single row per patient\n",
    "- After the aggregation, the table will be flattened, to encode the survey, drug class and week collected, in each column\n",
    "- Surveys are collected once a month and reflect the previous 30 days of drug use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define parameters for cleaning\n",
    "tfb_cols = ['patdeid','VISIT','TFB001A','TFB002A','TFB003A','TFB004A','TFB005A','TFB006A','TFB007A',\n",
    "            'TFB008A','TFB009A','TFB010A']\n",
    "tfb_labels = {'TFB001A':'survey_alcohol','TFB002A':'survey_cannabis','TFB003A':'survey_cocaine',    \n",
    "              'TFB010A':'survey_oxycodone','TFB009A':'survey_mmethadone','TFB004A':'survey_amphetamine','TFB005A':'survey_methamphetamine','TFB006A':'survey_opiates','TFB007A':'survey_benzodiazepines','TFB008A':'survey_propoxyphene'}\n",
    "\n",
    "# call the helper function to clean the data\n",
    "tfb = helper.clean_df(tfb, tfb_cols, tfb_labels)\n",
    "\n",
    "# visually inspect the data\n",
    "print('Shape of cleaned tfb dataframe is', tfb.shape)\n",
    "display(tfb[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregate rows by patient and visit, sum all records of drug use\n",
    "\n",
    "# create index\n",
    "index = ['patdeid','VISIT']\n",
    "\n",
    "# create aggregation dictionary, omit the first two columns, they do not require aggregation\n",
    "agg_dict = {col:'sum' for col in tfb.columns[2:]}\n",
    "\n",
    "# aggregate the data, we will apply sum to all instances of reported us to give the total use for the period\n",
    "tfb_agg = tfb.groupby(index).agg(agg_dict).reset_index()\n",
    "\n",
    "# visually inspect the data\n",
    "print('Aggregated tfb dataframe contains', tfb_agg.shape[0],'rows, coming from', tfb.shape[0],'rows')\n",
    "display(tfb_agg[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten the dataframe\n",
    "\n",
    "# set parameters to flatten survey data\n",
    "start = 0 # include data starting from week 0\n",
    "end = 24 # finish at week 24\n",
    "step = 4 # include data for every 4 weeks\n",
    "\n",
    "# call function to flatten dataframe\n",
    "tfb_flat = helper.flatten_dataframe(tfb_agg, start, end, step)\n",
    "\n",
    "# imputation strategy: fill missing values with 0, indicates no drug use\n",
    "tfb_flat.fillna(0, inplace=True)\n",
    "\n",
    "# visualize the data\n",
    "print('Flattended dataframe contains', tfb_flat.shape[1]-1,'features')\n",
    "display(tfb_flat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform the Medication Dose Table\n",
    "- This table has an issue with multiple rows per patient\n",
    "- Each dose of medication is recorded as a row\n",
    "- This means that if a patient received 7 doses of medication, there will be 7 rows for that patient\n",
    "- This needs to be consolidated into a single row per patient\n",
    "- For total_dose with null values, we will treat that as a no show or 0 dose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set parameters for cleaning the dataframe\n",
    "dos_cols = ['patdeid','VISIT','DOS002','DOS005']   \n",
    "dos_labels = {'DOS002':'medication','DOS005':'total_dose'}\n",
    "\n",
    "# call the helper function to clean the data\n",
    "dos = helper.clean_df(dos, dos_cols, dos_labels)\n",
    "\n",
    "# Imputation strategy: backfill and forwardfill missing values from medication and total dose\n",
    "dos['medication'] = dos['medication'].fillna(method='ffill').fillna(method='bfill')\n",
    "dos['total_dose'] = dos['total_dose'].fillna(method='ffill').fillna(method='bfill')\n",
    "\n",
    "# observe the data\n",
    "print('The medication dataframe contains', dos.shape[0],'rows that must be aggregated')\n",
    "display(dos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregate columns \n",
    "\n",
    "# create index\n",
    "index = ['patdeid','VISIT','medication']\n",
    "# create aggregation dictionary\n",
    "agg_dict = {col:'sum' for col in dos.columns[3:]}\n",
    "\n",
    "# aggregate the data, we will add daily dose to create weekly dose total, aggregating multiple columns per patient\n",
    "dos_agg = dos.groupby(index).agg(agg_dict).reset_index()\n",
    "\n",
    "# create df with patdeid and medication to merge later, this will help make analysis easier\n",
    "medication = dos[['patdeid', 'medication']].drop_duplicates(subset=['patdeid'], keep='first').reset_index(drop=True)\n",
    "\n",
    "# visualize the data\n",
    "print('Total rows in the aggregated dataframe:', dos_agg.shape[0],'from', dos.shape[0],'rows')\n",
    "dos_agg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering\n",
    "Create separate columns for bupe and methadone, this improves data quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature engineering\n",
    "\n",
    "# call helper function to create features from the medication data\n",
    "dos_agg = helper.med_features(dos_agg)\n",
    "\n",
    "# visually inspect the data\n",
    "print('The aggregated dataframe contains', dos_agg.shape[1]-2,'features')\n",
    "display(dos_agg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten the dataframe\n",
    "\n",
    "# set parameters to flatten the dataframe\n",
    "start = 0 # include data starting from week 0\n",
    "end = 24 # finish at week 24\n",
    "step = 1 # include data for every week\n",
    "\n",
    "# call function to flatten dataframe\n",
    "dos_flat = helper.flatten_dataframe(dos_agg, start, end, step)\n",
    "\n",
    "# imputation strategy: nulls come post merge, these were visits for patients who dropped out, fill with 0\n",
    "dos_flat.fillna(0, inplace=True)\n",
    "\n",
    "print('The flattened dataframe contains', dos_flat.shape[1]-1,'features')\n",
    "display(dos_flat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform the Clinical Opiate Withdrawal Scale Table\n",
    "There are 2 files that represent predose and postdose observation for patient withdrawal symptoms<br>\n",
    "There is one column per file with the score, so this is the most simple transformation<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set parameters for cleaning the dataframe\n",
    "cw1_cols = ['patdeid','COWS012']\n",
    "cw1_labels = {'COWS012':'cows_predose'}\n",
    "\n",
    "# call helper function to clean columns\n",
    "cw1 = helper.clean_df(cw1, cw1_cols, cw1_labels)\n",
    "\n",
    "cw1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set parameters for cleaning the dataframe\n",
    "cw2_cols = ['patdeid','COWS012']\n",
    "cw2_labels = {'COWS012':'cows_postdose'}\n",
    "\n",
    "# call helper function to clean columns\n",
    "cw2 = helper.clean_df(cw2, cw2_cols, cw2_labels)\n",
    "cw2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean the RBS table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set parameters for cleaning the dataframe\n",
    "rbs_cols = ['patdeid','RBS0A1B','RBS0A2B','RBS0A4B','RBS0A5B','RGRBS0C1']\n",
    "rbs_labels = {'RBS0A1B':'rbs_cocaine','RBS0A2B':'rbs_heroine','RBS0A4B':'rbs_other_opiates',\n",
    "              'RBS0A5B':'rbs_amphetamines','RGRBS0C1':'rbs_sexual_activity'}\n",
    "\n",
    "# call helper function to clean columns\n",
    "rbs = helper.clean_df(rbs, rbs_cols, rbs_labels)\n",
    "\n",
    "# values for 7 = refused and 9 = unknown, we will convert to 0\n",
    "rbs = rbs.replace({7:0, 9:0})\n",
    "\n",
    "# fill nan with 0 no response to survey\n",
    "rbs.fillna(0, inplace=True)\n",
    "\n",
    "rbs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregate Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The survey responses were recorded through multiple rows per patient\n",
    "# We will aggregate the data to create a single row per patient\n",
    "rbs = rbs.groupby('patdeid').sum().reset_index()\n",
    "print(f'The aggregated dataframe contains', rbs.shape[0],'rows and', rbs.shape[1],'columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge the Demographics Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dem = dem[['patdeid','DEM002']]\n",
    "\n",
    "dem = dem.rename(columns={'DEM002':'gender'})\n",
    "\n",
    "dem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we will merge all the tables into a single dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set parameters for merge\n",
    "\n",
    "# Define the dataframes to merge\n",
    "dfs = [rsa_flat, dos_flat, uds_flat, tfb_flat, \n",
    "       uds_features, dsm, mdh, pex, medication, \n",
    "       attendence, cw1, cw2, rbs, dem]\n",
    "\n",
    "# Initialize merged_df with the first DataFrame in the list\n",
    "merged_df = dfs[0]\n",
    "\n",
    "# Merge the dfs above using left merge on 'patdeid'\n",
    "for df in dfs[1:]:  # Start from the second item in the list\n",
    "    merged_df = pd.merge(merged_df, df, on='patdeid', how='left')\n",
    "\n",
    "# some rows were duplicated from one:many merge, they will be dropped\n",
    "merged_df = merged_df.drop_duplicates(subset=['patdeid'], keep='first')\n",
    "\n",
    "# Print the shape of the final dataframe\n",
    "print('The final table includes', merged_df.shape[1]-1, 'features for', merged_df.shape[0], 'patients in treatment')\n",
    "\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There are 666 patient instanes that dropped out week 1 that need further investigation\n",
    "- Of these instances, 67 patients attended week 1 of treatment and then dropped out\n",
    "- These instances will be preserved and added to the dataset\n",
    "- The remaining 599 patients did not record any clinical data, so these will be dropped\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter patient ids, by those that attended the first week and received medication, drop the remaining rows\n",
    "# create a list with the indicies of rows to keep\n",
    "nan_df = merged_df.loc[merged_df['weeks_attended']==1][['meds_methadone_0','meds_buprenorphine_0']].dropna()\n",
    "\n",
    "# indices to keep\n",
    "indices_to_keep = nan_df.index\n",
    "\n",
    "# create new_df for patients who dropped out week 1\n",
    "keep_rows = merged_df[merged_df['weeks_attended'] == 1]\n",
    "\n",
    "# use the indices to keep to filter the merged_df\n",
    "keep_rows = merged_df.loc[indices_to_keep]\n",
    "\n",
    "keep_rows.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge preserved patient instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove rows where patients attended only one week from merged_df\n",
    "new_df = merged_df.loc[merged_df['weeks_attended'] != 1]\n",
    "\n",
    "# concat both dataframes\n",
    "new_df = pd.concat([new_df, keep_rows])\n",
    "\n",
    "# review the value counts to confirm transformation\n",
    "print('new_df shape:', new_df.shape)\n",
    "display(new_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create heatmap for cows_predose and cows_postdose\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(new_df[['cows_predose', 'cows_postdose']].isnull(), cbar=False)\n",
    "plt.title('Missing values in the dataset')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.loc[new_df.cows_postdose.isnull()][['cows_postdose','weeks_attended']].loc[new_df.weeks_attended == 1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations from Missing Data\n",
    "Given the healthcare context, the way in which missing data is handled is critical<br>\n",
    "Removing rows with missing data, would create too much bias in the dataset<br>\n",
    "We are also working with a small number of samples, it's important to preserve as much data as possible<br>\n",
    "<br>\n",
    "There are roughly 1,400 nan values that require strategy for imputation<br>\n",
    "There are a few columns that show concern, listed as follows:<br>\n",
    "- `medication` - there were 7 patient instances without data, we will inpute to 0 dose, for not attending<br>\n",
    "but we will preserve their records as the data for their treatment is still valuable<br>\n",
    "- `surveys` - there were 11 patient instances with missing surveys, 6 of these patient dropped out in week 1<br>\n",
    "We will inpute with the mean value for surveys of that week, as the asumption is that the patient did used\n",
    "drugs and did not submit a survey<br>\n",
    "- `pex` - physical exam - there was 1 patient missing exam, we will inpute as not present\n",
    "- `dsm` - addiction diagnosis - there were 4 patients missing addiction diagnosis, we will inpute as not evaluated\n",
    "-`rbs` - risk based survey, there were 3 patients missing this survey, we will inpute with mean value for the assessment\n",
    "\n",
    "\n",
    "The clinical opiate withdrawal scale has a high number of missing values<br>\n",
    "- `cows_predose` - 33\n",
    "- `cows_postdose` - 171 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Patient Nulls for Surveys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create survey df to evaluate nulls\n",
    "survey = new_df[[col for col in new_df.columns if 'survey' in col]]\n",
    "\n",
    "\n",
    "# show the nulls in the survey dataframe\n",
    "index = survey[survey.isnull().any(axis=1)].index\n",
    "\n",
    "# observe the patient instances with all the data\n",
    "new_df.loc[index]['dropout'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Patient Nulls for Medication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create meds DF to evaluate nulls\n",
    "meds = new_df[[col for col in new_df.columns if 'meds' in col]]\n",
    "\n",
    "# show the rows with nan values\n",
    "meds[meds.isnull().any(axis=1)].index\n",
    "\n",
    "# create index for reference on new_df\n",
    "index = [202, 490, 761, 1132, 1259, 1554, 2022]\n",
    "\n",
    "\n",
    "# observe the patient instances with all the data\n",
    "new_df.loc[index]['dropout'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Patient Nulls for Medication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pex = new_df[[col for col in new_df.columns if 'pex' in col]]\n",
    "\n",
    "index = pex[pex.isnull().any(axis=1)].index\n",
    "\n",
    "new_df.loc[index]['dropout'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Patient Nulls for Physical Exam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdh = new_df[[col for col in new_df.columns if 'mdh' in col]]\n",
    "\n",
    "mdh[mdh.isnull().any(axis=1)]\n",
    "\n",
    "new_df.loc[2022]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Patient Nulls for DSM Diagnosis  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsm = new_df[[col for col in new_df.columns if 'dsm' in col]]\n",
    "\n",
    "index = dsm[dsm.isnull().any(axis=1)].index\n",
    "\n",
    "new_df.loc[index]['dropout'].value_counts() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Patient Nulls for COWS Predose and Postdose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cows = new_df[[col for col in new_df.columns if 'cows' in col]]\n",
    "\n",
    "# look at predose nulls\n",
    "predose_index = cows[cows.cows_predose.isnull()].index\n",
    "\n",
    "# look at postdose nulls\n",
    "postdose_index = cows[cows.cows_postdose.isnull()].index\n",
    "\n",
    "# look at the rows with nulls\n",
    "new_df.loc[predose_index]['dropout'].value_counts() \n",
    "\n",
    "new_df.loc[postdose_index]['dropout'].value_counts()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate RBS Nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbs = new_df[[col for col in new_df.columns if 'rbs' in col]]\n",
    "\n",
    "index  = rbs[rbs.isnull().any(axis=1)].index\n",
    "\n",
    "new_df.loc[index]['dropout'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputation Execution\n",
    "- `medication` - 7 instances inpute with 0 dose\n",
    "- `surveys` - 11 instances inpute with mean value for that week\n",
    "- `pex` Physical Exam - 1 instance inpute as not present\n",
    "- `dsm` Addiction diagnosis - 4 instances inpute as not evaluated\n",
    "- `cows_predose` - 33 instances, we will inpute with linear regression\n",
    "- `cows_postdose` - 171 instances, we will inpute with linear regression\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run a loop to fill nulls with simple replacement\n",
    "for col in new_df.columns:\n",
    "    if 'meds' in col:\n",
    "        new_df[col] = new_df[col].fillna(0)\n",
    "    elif 'medication':\n",
    "        new_df[col] = new_df[col].fillna(0)\n",
    "    elif 'survey' in col:\n",
    "        new_df[col] = new_df[col].fillna(new_df[col].mean())\n",
    "    elif 'pex' in col:\n",
    "        new_df[col] = new_df[col].fillna('not_evaluated')\n",
    "    elif 'mdh' in col:\n",
    "        new_df[col] = new_df[col].fillna('not_evaluated')\n",
    "    elif 'dsm' in col:\n",
    "        new_df[col] = new_df[col].fillna('not_evaluated')\n",
    "    elif 'cows' in col:\n",
    "        new_df[col] = new_df[col].fillna(0)\n",
    "    elif 'rbs' in col:\n",
    "        new_df[col] = new_df[col].fillna(new_df[col].mean())\n",
    "    elif 'cows_predose' in col:\n",
    "        new_df[col] = new_df[col].fillna(0)\n",
    "    elif 'cows_postdose' in col:\n",
    "        new_df[col] = new_df[col].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will use linear regression to impute the missing values for COWS Predose and Postdose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use iterative imputer for cows_predose \n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "# create an instance of the imputer\n",
    "imputer = IterativeImputer(max_iter=10, random_state=0)\n",
    "\n",
    "# fit the imputer\n",
    "imputer.fit(new_df[['cows_predose']])\n",
    "\n",
    "# transform the data\n",
    "new_df['cows_predose'] = imputer.transform(new_df[['cows_predose']])\n",
    "\n",
    "# visually inspect the data\n",
    "new_df['cows_predose'].hist()\n",
    "plt.title('COWS Predose')\n",
    "plt.xlabel('Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an instance of the imputer\n",
    "imputer = IterativeImputer(max_iter=10, random_state=0)\n",
    "\n",
    "# fit the imputer\n",
    "imputer.fit(new_df[['cows_postdose']])\n",
    "\n",
    "# transform the data\n",
    "new_df['cows_postdose'] = imputer.transform(new_df[['cows_postdose']])\n",
    "\n",
    "# visually inspect the data\n",
    "new_df['cows_postdose'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "new_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to data folder in csv\n",
    "new_df.to_csv('../data/new_merged_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_features = pd.read_csv('../data/benchmark_features.csv')\n",
    "\n",
    "benchmark_features = benchmark_features.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = new_df.loc[:, benchmark_features]\n",
    "\n",
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot heatmap of nulls\n",
    "plt.figure(figsize=(14, 6))\n",
    "sns.heatmap(new_df.isnull(), cbar=False)\n",
    "plt.title('Missing values in the dataset')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(14, 6))\n",
    "new_df.cows_predose.hist(alpha=0.5, ax=ax[0])\n",
    "new_df.cows_postdose.hist(alpha=0.5, ax=ax[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "new_df.loc[new_df.cows_postdose.isnull()]['dropout'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show heatmap for cows predose and postdose\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(new_df[['cows_predose', 'cows_postdose']].isnull(), cbar=False)\n",
    "plt.title('Missing values in the dataset')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
