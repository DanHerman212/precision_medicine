{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning Strategy\n",
    "For this project, we will use the CTN0027 Dataset from the clinical trial network.<br>\n",
    "The dataset and all it's documentation are avaialble at the following website:<br>\n",
    "https://datashare.nida.nih.gov/study/nida-ctn-0027<br>\n",
    "\n",
    "There are a few challenges to highlight that should be considered in the cleaning approach:\n",
    "- Large de-identified dataset must be manually labeled, very time consuming and prone to errors\n",
    "- High dimension data - Requires bespoke transformations to fit into machine learning models\n",
    "\n",
    "## Tables to be cleaned\n",
    "| File Name | Table Name | Variable |Description | Process Applied |\n",
    "| :--- | :--- | :--- | :--- | :--- |\n",
    "| T_FRRSA.csv | Research Session Attendance|RSA |Records attendence for each week of treatment | Clean, Flatten, Feature Extraction, Merge |\n",
    "| T_FRDEM.csv | Demographics|DEM |Sex, Ethnicity, Race | Clean, Merge |\n",
    "| T_FRUDSAB.csv | Urine Drug Screen| UDS  |Drug test for 8 different drug classes, taken weekly for 24 weeks | Clean, Flatten, Feature Extraction, Merge |\n",
    "| T_FRDSM.csv | DSM-IV Diagnosis|DSM |Tracks clinical diagnosis for substance use disorder, in accordance with DSM guidelines| Clean, Merge |\n",
    "| T_FRMDH.csv | Medical and Psychiatric History|MDH |Tracks medical and psychiatric history of 24 different Conditions| Clean, Merge |\n",
    "| T_FRPEX.csv | Physical Exam|PEX |Tracks the appearance and condition of patients for 12 different physical observations| Clean, Merge |\n",
    "| T_FRTFB.csv | Timeline Follow Back Survey|TFB |Surveys for self reported drug use, collected every 4 weeks, includes previous 30 days of use ot week 0, 4, 8, 12, 16, 20, 24| Clean, Aggregate, Flatten, Merge |\n",
    "|T_FRDOS.csv | Dose Record |DOS |Records the medication, averge weekly dose and week of treatment| Clean, Aggregate, Feature Extraction, Flatten, Merge |\n",
    "|T_FRCOWS.csv| Clinical Opiate Withdrawal Scale (Predose)|CW1 |Records the severity of opiate withdrawal symptoms, taken at assessment| Clean, Merge |\n",
    "|T_FRCOWS2.csv| Clinical Opiate Withdrawal Scale (Postdose)|CW2 |Records the severity of opiate withdrawal symptoms, taken after dose| Clean, Merge |\n",
    "\n",
    "\n",
    "## Data Cleaning Process\n",
    "We will try to keep things simple and employ a process driven by reusable functions<br>\n",
    "to **improve data quality**, **reduce time to market** and **reducing human error**.<br>\n",
    "<br> \n",
    "For each table we will follow the following steps:<br>\n",
    "1. Load the data\n",
    "2. Identify columns that require labels\n",
    "3. Apply labels to columns\n",
    "4. Drop columns that are not needed\n",
    "5. Create imputation strategy for missing values\n",
    "5. Apply transformations to values where required\n",
    "3. Feature Engineering (if necessary)\n",
    "4. Flatten Dataframes (encode week of treatment into columns, where applicable)\n",
    "4. Merge with other tables\n",
    "\n",
    "## List of Reusable Functions\n",
    "| Name of Function | Description | \n",
    "| ---------------- | ----------- |\n",
    "| clean_df | Clean the given DataFrame by dropping unnecessary columns, renaming columns, and reordering columns. |\n",
    "| flatten_dataframe | This function creates features by combining the VISIT column with the clinical datapoint (see example below).  The goal is to reduce individual rows per patient.  The data currently presents 25 rows per patient (for each week of treatment), which won't work for machine learning.  The model will only accept one row per patient, so we must encode all the clinical data into columns.  We will tranform the data by creating a separate dataframe for each week of treatment.  We will encode the week of treatment into the columns in each dataframe and then merge them together to form a high quality dataset, with granular level treatment data, that should help improve machine learning model accuracy.  This is a complex transformation, but justified for the incremental improvement to machine learning accuracy |\n",
    "| merge_dfs | Merge the given list of DataFrames into one DataFrame. |\n",
    "| uds_features | Creates 4 new features which are metrics used to measure outcomes from opiate test data. |\n",
    "| med_features | Creates 2 new features for medication dose to enrich dataset and improve accuracy in machine learning|\n",
    "\n",
    "### Example of How Flattening Works\n",
    "![flatten](../images/flatten.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # data manipulation library\n",
    "import numpy as np # numerical computing library\n",
    "import matplotlib.pyplot as plt # data visualization library\n",
    "import seaborn as sns # advanced data visualization library\n",
    "import helper # custom fuctions I created to clean and plot data\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Data\n",
    "We will load 10 files from the de-identified dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rsa shape: (27029, 12)\n",
      "dem shape: (1920, 43)\n",
      "uds shape: (24930, 66)\n",
      "dsm shape: (1889, 26)\n",
      "mdh shape: (1869, 89)\n",
      "pex shape: (2779, 33)\n",
      "tfb shape: (100518, 56)\n",
      "dos shape: (160908, 19)\n",
      "cw1 shape: (1315, 24)\n",
      "cw2 shape: (1316, 24)\n"
     ]
    }
   ],
   "source": [
    "# define parameters to load data\n",
    "\n",
    "# define the path to the data\n",
    "data_path = '../unlabeled_data/'\n",
    "\n",
    "# define the names of the files to load\n",
    "file_names = ['T_FRRSA.csv', 'T_FRDEM.csv','T_FRUDSAB.csv',\n",
    "              'T_FRDSM.csv','T_FRMDH.csv','T_FRPEX.csv',\n",
    "              'T_FRTFB.csv','T_FRDOS.csv','T_FRCOWS.csv', 'T_FRCOWS2.csv']\n",
    "\n",
    "# define the names of the variables for the dataframes\n",
    "variables = ['rsa', 'dem', 'uds', 'dsm', 'mdh', 'pex', \n",
    "             'tfb', 'dos', 'cw1', 'cw2']\n",
    "\n",
    "# create a loop to iterate through the files and load them into the notebook\n",
    "for file_name, variable in zip(file_names, variables):\n",
    "        globals()[variable] = pd.read_csv(data_path + file_name)\n",
    "        print(f\"{variable} shape: {globals()[variable].shape}\") # print the shape of the dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform Attendence Table\n",
    "- This table establishes the patient population and will serve as the primary table\n",
    "- All subsequent tables will use a LEFT JOIN to add clinical data as columns to each patient ID\n",
    "- This table requires feature engineering for `attendance` and `dropout` variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25712, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patdeid</th>\n",
       "      <th>VISIT</th>\n",
       "      <th>rsa_week</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27024</th>\n",
       "      <td>1931</td>\n",
       "      <td>28</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27025</th>\n",
       "      <td>1931</td>\n",
       "      <td>32</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27026</th>\n",
       "      <td>1932</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27027</th>\n",
       "      <td>1933</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27028</th>\n",
       "      <td>1934</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25712 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       patdeid  VISIT  rsa_week\n",
       "0            1      0       1.0\n",
       "2            1      1       1.0\n",
       "3            1      2       1.0\n",
       "4            1      3       1.0\n",
       "5            1      4       1.0\n",
       "...        ...    ...       ...\n",
       "27024     1931     28       1.0\n",
       "27025     1931     32       1.0\n",
       "27026     1932      0       1.0\n",
       "27027     1933      0       1.0\n",
       "27028     1934      0       1.0\n",
       "\n",
       "[25712 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# we will define the columns and labels that we need for each df and then transform the data\n",
    "\n",
    "# set parameters for transformation\n",
    "rsa_cols = ['patdeid','VISIT','RSA001']\n",
    "rsa_labels = {'RSA001':'rsa_week'}\n",
    "\n",
    "# the helper function will transform the data\n",
    "rsa = helper.clean_df(rsa, rsa_cols, rsa_labels)\n",
    "\n",
    "# fill nulls with 0, marking no attendance\n",
    "\n",
    "\n",
    "# remove the followup visits from the main clinical data weeks 0 - 24\n",
    "# rsa = rsa[~rsa['VISIT'].isin([28, 32])]\n",
    "\n",
    "# remove duplicate rows\n",
    "rsa = rsa.drop_duplicates(subset=['patdeid', 'VISIT'], keep='first')\n",
    "\n",
    "# observe shape and sample 5 observations\n",
    "print(rsa.shape)\n",
    "display(rsa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering\n",
    "Capture weeks completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patdeid</th>\n",
       "      <th>weeks_comp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1915</th>\n",
       "      <td>1930</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1916</th>\n",
       "      <td>1931</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1917</th>\n",
       "      <td>1932</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1918</th>\n",
       "      <td>1933</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1919</th>\n",
       "      <td>1934</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1920 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      patdeid  weeks_comp\n",
       "0           1          27\n",
       "1           2          27\n",
       "2           3          27\n",
       "3           4          27\n",
       "4           5           1\n",
       "...       ...         ...\n",
       "1915     1930           1\n",
       "1916     1931          27\n",
       "1917     1932           1\n",
       "1918     1933           1\n",
       "1919     1934           1\n",
       "\n",
       "[1920 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create df with count of attendance for each patient\n",
    "attendence = rsa.groupby('patdeid')['rsa_week'].size().to_frame('weeks_comp').reset_index()\n",
    "\n",
    "attendence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patdeid</th>\n",
       "      <th>rsa_week_0</th>\n",
       "      <th>rsa_week_1</th>\n",
       "      <th>rsa_week_2</th>\n",
       "      <th>rsa_week_3</th>\n",
       "      <th>rsa_week_4</th>\n",
       "      <th>rsa_week_5</th>\n",
       "      <th>rsa_week_6</th>\n",
       "      <th>rsa_week_7</th>\n",
       "      <th>rsa_week_8</th>\n",
       "      <th>...</th>\n",
       "      <th>rsa_week_16</th>\n",
       "      <th>rsa_week_17</th>\n",
       "      <th>rsa_week_18</th>\n",
       "      <th>rsa_week_19</th>\n",
       "      <th>rsa_week_20</th>\n",
       "      <th>rsa_week_21</th>\n",
       "      <th>rsa_week_22</th>\n",
       "      <th>rsa_week_23</th>\n",
       "      <th>rsa_week_24</th>\n",
       "      <th>rsa_week_28</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1915</th>\n",
       "      <td>1930</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1916</th>\n",
       "      <td>1931</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1917</th>\n",
       "      <td>1932</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1918</th>\n",
       "      <td>1933</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1919</th>\n",
       "      <td>1934</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1920 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      patdeid  rsa_week_0  rsa_week_1  rsa_week_2  rsa_week_3  rsa_week_4  \\\n",
       "0           1         1.0         1.0         1.0         1.0         1.0   \n",
       "1           2         1.0         1.0         1.0         1.0         1.0   \n",
       "2           3         1.0         1.0         1.0         1.0         1.0   \n",
       "3           4         1.0         1.0         1.0         0.0         1.0   \n",
       "4           5         1.0         0.0         0.0         0.0         0.0   \n",
       "...       ...         ...         ...         ...         ...         ...   \n",
       "1915     1930         1.0         0.0         0.0         0.0         0.0   \n",
       "1916     1931         1.0         1.0         1.0         1.0         1.0   \n",
       "1917     1932         1.0         0.0         0.0         0.0         0.0   \n",
       "1918     1933         1.0         0.0         0.0         0.0         0.0   \n",
       "1919     1934         1.0         0.0         0.0         0.0         0.0   \n",
       "\n",
       "      rsa_week_5  rsa_week_6  rsa_week_7  rsa_week_8  ...  rsa_week_16  \\\n",
       "0            1.0         1.0         1.0         1.0  ...          0.0   \n",
       "1            1.0         1.0         1.0         1.0  ...          1.0   \n",
       "2            1.0         1.0         1.0         1.0  ...          1.0   \n",
       "3            1.0         1.0         1.0         1.0  ...          1.0   \n",
       "4            0.0         0.0         0.0         0.0  ...          0.0   \n",
       "...          ...         ...         ...         ...  ...          ...   \n",
       "1915         0.0         0.0         0.0         0.0  ...          0.0   \n",
       "1916         1.0         1.0         1.0         1.0  ...          1.0   \n",
       "1917         0.0         0.0         0.0         0.0  ...          0.0   \n",
       "1918         0.0         0.0         0.0         0.0  ...          0.0   \n",
       "1919         0.0         0.0         0.0         0.0  ...          0.0   \n",
       "\n",
       "      rsa_week_17  rsa_week_18  rsa_week_19  rsa_week_20  rsa_week_21  \\\n",
       "0             1.0          1.0          1.0          1.0          1.0   \n",
       "1             1.0          1.0          1.0          1.0          1.0   \n",
       "2             1.0          1.0          1.0          1.0          1.0   \n",
       "3             1.0          1.0          1.0          1.0          1.0   \n",
       "4             0.0          0.0          0.0          0.0          0.0   \n",
       "...           ...          ...          ...          ...          ...   \n",
       "1915          0.0          0.0          0.0          0.0          0.0   \n",
       "1916          0.0          1.0          1.0          1.0          1.0   \n",
       "1917          0.0          0.0          0.0          0.0          0.0   \n",
       "1918          0.0          0.0          0.0          0.0          0.0   \n",
       "1919          0.0          0.0          0.0          0.0          0.0   \n",
       "\n",
       "      rsa_week_22  rsa_week_23  rsa_week_24  rsa_week_28  \n",
       "0             1.0          1.0          1.0          1.0  \n",
       "1             1.0          1.0          1.0          1.0  \n",
       "2             1.0          1.0          1.0          1.0  \n",
       "3             1.0          0.0          1.0          1.0  \n",
       "4             0.0          0.0          0.0          0.0  \n",
       "...           ...          ...          ...          ...  \n",
       "1915          0.0          0.0          0.0          0.0  \n",
       "1916          1.0          0.0          1.0          1.0  \n",
       "1917          0.0          0.0          0.0          0.0  \n",
       "1918          0.0          0.0          0.0          0.0  \n",
       "1919          0.0          0.0          0.0          0.0  \n",
       "\n",
       "[1920 rows x 27 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set parameters to flatten the df\n",
    "start = 0 # include data starting from week 0\n",
    "end = 28 # finish at week 24\n",
    "step = 1 # include data for every week\n",
    "\n",
    "# call function to flatten dataframe\n",
    "rsa_flat = helper.flatten_dataframe(rsa, start, end, step)\n",
    "\n",
    "# fill nulls with 0 for no attendance\n",
    "rsa_flat = rsa_flat.fillna(0)\n",
    "\n",
    "# there is a followup visit for week 32\n",
    "# the flatten_dataframes() function will complete a sequence\n",
    "# will columns between weeks 25 - 27  that are empty\n",
    "# we will drop these columns\n",
    "columns_to_drop = [f'rsa_week_{i}' for i in range(25, 28)]\n",
    "rsa_flat = rsa_flat.drop(columns=columns_to_drop)\n",
    "\n",
    "# visually inspect the data\n",
    "rsa_flat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering\n",
    "- Week 28 is a followup week to measure paitent dropout.\n",
    "- If patients do not attend week 28, they are considered to have dropped out of the program\n",
    "- We will rename rsa_week_28 to `dropout` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dropout\n",
       "0.0    1295\n",
       "1.0     625\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rename rsa_week_28 to dropout\n",
    "rsa_flat = rsa_flat.rename(columns={'rsa_week_28':'dropout'})\n",
    "\n",
    "rsa_flat.dropout.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform Demographics Table\n",
    "This table came with mixed numbering convensions for values.<br>\n",
    "For gender and ethnicity, the responses are categorical strings<br>\n",
    "For race the responses are numeric and binary<br>\n",
    "After the table is transformed, we will have to melt the 'race' columns to a single column<br>\n",
    "This transformation is required to enable machine learning pipelines to work with the data<bR>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patdeid</th>\n",
       "      <th>dem_gender</th>\n",
       "      <th>dem_ethnicity</th>\n",
       "      <th>dem_race_no_answer</th>\n",
       "      <th>dem_race_unknown</th>\n",
       "      <th>dem_race_amer_ind</th>\n",
       "      <th>dem_race_asian</th>\n",
       "      <th>dem_race_black</th>\n",
       "      <th>dem_race_pacific_islander</th>\n",
       "      <th>dem_race_white</th>\n",
       "      <th>dem_race_other</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>male</td>\n",
       "      <td>not_spanish_origin</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>male</td>\n",
       "      <td>not_spanish_origin</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>not_spanish_origin</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>female</td>\n",
       "      <td>not_spanish_origin</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>male</td>\n",
       "      <td>not_spanish_origin</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1915</th>\n",
       "      <td>1930</td>\n",
       "      <td>female</td>\n",
       "      <td>not_spanish_origin</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1916</th>\n",
       "      <td>1931</td>\n",
       "      <td>male</td>\n",
       "      <td>not_spanish_origin</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1917</th>\n",
       "      <td>1932</td>\n",
       "      <td>female</td>\n",
       "      <td>not_spanish_origin</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1918</th>\n",
       "      <td>1933</td>\n",
       "      <td>male</td>\n",
       "      <td>not_spanish_origin</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1919</th>\n",
       "      <td>1934</td>\n",
       "      <td>male</td>\n",
       "      <td>not_spanish_origin</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1920 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      patdeid dem_gender       dem_ethnicity  dem_race_no_answer  \\\n",
       "0           1       male  not_spanish_origin                 0.0   \n",
       "1           2       male  not_spanish_origin                 0.0   \n",
       "2           3       male  not_spanish_origin                 0.0   \n",
       "3           4     female  not_spanish_origin                 0.0   \n",
       "4           5       male  not_spanish_origin                 0.0   \n",
       "...       ...        ...                 ...                 ...   \n",
       "1915     1930     female  not_spanish_origin                 0.0   \n",
       "1916     1931       male  not_spanish_origin                 0.0   \n",
       "1917     1932     female  not_spanish_origin                 0.0   \n",
       "1918     1933       male  not_spanish_origin                 0.0   \n",
       "1919     1934       male  not_spanish_origin                 0.0   \n",
       "\n",
       "      dem_race_unknown  dem_race_amer_ind  dem_race_asian  dem_race_black  \\\n",
       "0                  0.0                0.0             0.0             0.0   \n",
       "1                  0.0                0.0             0.0             0.0   \n",
       "2                  0.0                0.0             0.0             0.0   \n",
       "3                  0.0                0.0             0.0             0.0   \n",
       "4                  0.0                0.0             0.0             0.0   \n",
       "...                ...                ...             ...             ...   \n",
       "1915               0.0                0.0             0.0             0.0   \n",
       "1916               0.0                0.0             0.0             0.0   \n",
       "1917               0.0                0.0             0.0             0.0   \n",
       "1918               0.0                0.0             0.0             0.0   \n",
       "1919               0.0                0.0             0.0             0.0   \n",
       "\n",
       "      dem_race_pacific_islander  dem_race_white  dem_race_other  \n",
       "0                           0.0             1.0             0.0  \n",
       "1                           0.0             1.0             0.0  \n",
       "2                           0.0             1.0             0.0  \n",
       "3                           0.0             1.0             0.0  \n",
       "4                           0.0             1.0             0.0  \n",
       "...                         ...             ...             ...  \n",
       "1915                        0.0             1.0             0.0  \n",
       "1916                        0.0             1.0             0.0  \n",
       "1917                        0.0             1.0             0.0  \n",
       "1918                        0.0             1.0             0.0  \n",
       "1919                        0.0             1.0             0.0  \n",
       "\n",
       "[1920 rows x 11 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set parameters for transformation\n",
    "dem_cols = ['patdeid','DEM002','DEM003A','DEM004A','DEM004B','DEM004C','DEM004D','DEM004E',\n",
    "            'DEM004F','DEM004G','DEM004H']\n",
    "dem_labels = {'DEM002':'dem_gender','DEM003A':'dem_ethnicity','DEM004A':'dem_race_amer_ind',\n",
    "              'DEM004B':'dem_race_asian','DEM004C':'dem_race_black','DEM004D':'dem_race_pacific_islander',\n",
    "             'DEM004E':'dem_race_white','DEM004F':'dem_race_other','DEM004G':'dem_race_no_answer',\n",
    "                'DEM004H':'dem_race_unknown'}\n",
    "\n",
    "# the helper function will clean and transform the data\n",
    "dem = helper.clean_df(dem, dem_cols, dem_labels)\n",
    "\n",
    "# for ethnicity column, map 1:'spanish_origin', 2:'not_spanish_origin', to values\n",
    "for col in dem.columns:\n",
    "    if col =='dem_ethnicity':\n",
    "        dem[col] = dem[col].replace({1:'spanish_origin',2:'not_spanish_origin'})\n",
    "    if col =='dem_gender':\n",
    "        dem[col] = dem[col].replace({1:'male',2:'female'})\n",
    "    if col =='dem_race_asian':\n",
    "        dem[col] = dem[col].replace({2:1})\n",
    "    if col=='dem_race_black':\n",
    "        dem[col] = dem[col].replace({3:1})\n",
    "    if col=='dem_race_pacific_islander':\n",
    "        dem[col] = dem[col].replace({4:1})\n",
    "    if col=='dem_race_white':\n",
    "        dem[col] = dem[col].replace({5:1})\n",
    "    if col=='dem_race_other':\n",
    "        dem[col] = dem[col].replace({6:1})\n",
    "    if col=='dem_race_no_answer':\n",
    "        dem[col] = dem[col].replace({7:1})\n",
    "    if col=='dem_race_unknown':\n",
    "        dem[col] = dem[col].replace({8:1})\n",
    "\n",
    "# imputation strategy: 0 for missing values, purpose is for counts of dem data\n",
    "dem = dem.fillna(0)\n",
    "\n",
    "# review the data\n",
    "dem\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Melt the 'race' columns into a single column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patdeid</th>\n",
       "      <th>dem_gender</th>\n",
       "      <th>dem_ethnicity</th>\n",
       "      <th>dem_race</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>male</td>\n",
       "      <td>not_spanish_origin</td>\n",
       "      <td>white</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>male</td>\n",
       "      <td>not_spanish_origin</td>\n",
       "      <td>white</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>not_spanish_origin</td>\n",
       "      <td>white</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>female</td>\n",
       "      <td>not_spanish_origin</td>\n",
       "      <td>white</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>male</td>\n",
       "      <td>not_spanish_origin</td>\n",
       "      <td>white</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1915</th>\n",
       "      <td>1930</td>\n",
       "      <td>female</td>\n",
       "      <td>not_spanish_origin</td>\n",
       "      <td>white</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1916</th>\n",
       "      <td>1931</td>\n",
       "      <td>male</td>\n",
       "      <td>not_spanish_origin</td>\n",
       "      <td>white</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1917</th>\n",
       "      <td>1932</td>\n",
       "      <td>female</td>\n",
       "      <td>not_spanish_origin</td>\n",
       "      <td>white</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1918</th>\n",
       "      <td>1933</td>\n",
       "      <td>male</td>\n",
       "      <td>not_spanish_origin</td>\n",
       "      <td>white</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1919</th>\n",
       "      <td>1934</td>\n",
       "      <td>male</td>\n",
       "      <td>not_spanish_origin</td>\n",
       "      <td>white</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1920 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      patdeid dem_gender       dem_ethnicity dem_race\n",
       "0           1       male  not_spanish_origin    white\n",
       "1           2       male  not_spanish_origin    white\n",
       "2           3       male  not_spanish_origin    white\n",
       "3           4     female  not_spanish_origin    white\n",
       "4           5       male  not_spanish_origin    white\n",
       "...       ...        ...                 ...      ...\n",
       "1915     1930     female  not_spanish_origin    white\n",
       "1916     1931       male  not_spanish_origin    white\n",
       "1917     1932     female  not_spanish_origin    white\n",
       "1918     1933       male  not_spanish_origin    white\n",
       "1919     1934       male  not_spanish_origin    white\n",
       "\n",
       "[1920 rows x 4 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# subset the race columns, skip the first three columns\n",
    "dem_race = dem.iloc[:,3:]\n",
    "\n",
    "# trim the prefix 'dem_race_' and leave the race only, we will transpose from columns to rows next\n",
    "dem_race.columns = dem_race.columns.str.replace('dem_race_','')\n",
    "\n",
    "# use IDXMAX to find the column with the highest value and transpose to rows in series\n",
    "# use .to_frame() method to transform from series dataframe to merge with other columns\n",
    "melt_dem = dem_race.idxmax(axis=1).to_frame('dem_race')\n",
    "\n",
    "# subset the first three columns of the original dem dataframe for merge\n",
    "dem = dem.iloc[:,:3]\n",
    "\n",
    "# merge the dataframes\n",
    "dem = pd.concat([dem, melt_dem], axis=1)\n",
    "\n",
    "dem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform Urine Drug Screen Table\n",
    "This table contains the data for most of the outcome metrics<br>\n",
    "Stay tuned for feature engineering section towards the end of this table transformation<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set parameters for transformation\n",
    "uds_cols = ['patdeid','VISIT', 'UDS005', 'UDS006', 'UDS007', 'UDS008', 'UDS009', 'UDS010', 'UDS011', 'UDS012', \n",
    "            'UDS013']\n",
    "uds_labels = {'UDS005':'test_Amphetamines', 'UDS006':'test_Benzodiazepines','UDS007':'test_MMethadone', \n",
    "              'UDS008':'test_Oxycodone', 'UDS009':'test_Cocaine', 'UDS010':'test_Methamphetamine', 'UDS011':'test_Opiate300', 'UDS012':'test_Cannabinoids', 'UDS013':'test_Propoxyphene'}\n",
    "\n",
    "# the helper function will clean and transform the data\n",
    "uds = helper.clean_df(uds, uds_cols, uds_labels)\n",
    "\n",
    "print('Dataframe uds with shape of', uds.shape, 'has been cleaned')\n",
    "display(uds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe is ready to be flattened\n",
    "\n",
    "# set params for flattening\n",
    "start = 0 # include data starting from week 0\n",
    "end = 24 # finish at week 24\n",
    "step = 1 # include data for every week\n",
    "\n",
    "# call function to flatten dataframe\n",
    "uds_flat = helper.flatten_dataframe(uds, start, end, step)\n",
    "\n",
    "# fill missing values with 1, which is a binary value for positive test\n",
    "uds_flat.fillna(1, inplace=True)\n",
    "\n",
    "# visually inspect the data\n",
    "print('The clinical data was added in the form of',uds_flat.shape[1],'features')\n",
    "print('Which includes tests for 8 different drug classes over 24 weeks')\n",
    "display(uds_flat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering\n",
    "The following metrics will be created to assess treatment success:<br>\n",
    "- `TNT` - (numeric) - total negative tests, a measure of clinical benefit, count of negative tests over 24 weeks\n",
    "- `NTR` - (float) - negative test rate, a measure of clinical benefit, percentage of negative tests over 24 weeks\n",
    "- `CNT` - (numeric) - concsecutive negative tests, a measure of clinical benefit, count of consecutive negative tests over 24 weeks\n",
    "- `responder` - (binary) - indicating if the patient responds to treatment, by testing negative for opiates for the final 4 weeks of treatment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call the helper function to create the UDS features\n",
    "uds_features = helper.uds_features(uds_flat)\n",
    "\n",
    "# isolate the features df for merge with the clinical data\n",
    "uds_features = uds_features[['patdeid','TNT','NTR','CNT','responder']]\n",
    "\n",
    "print('The UDS features have been created with shape of', uds_features.shape)\n",
    "display(uds_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform DSM-IV Diagnosis Table\n",
    "The values for these features are mapped as follows:<br>\n",
    "<br>\n",
    "1 = Dependence<br>\n",
    "2 = Abuse<br>\n",
    "3 = No Diagnosis<br>\n",
    "<br>\n",
    "This will require one hot encoding in the datapipelines later on.<br>\n",
    "We will label the values as text strings, so that they can appear<br>\n",
    "on columns in the final dataset. The text strings will also help<br>\n",
    "with analysis<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set params for transformation\n",
    "dsm_cols = ['patdeid','DSMOPI','DSMAL','DSMAM','DSMCA','DSMCO','DSMSE']\n",
    "dsm_labels = {'DSMOPI':'dsm_opiates','DSMAL':'dsm_alcohol','DSMAM':'dsm_amphetamine',\n",
    "              'DSMCA':'dsm_cannabis','DSMCO':'dsm_cocaine','DSMSE':'dsm_sedative'}\n",
    "\n",
    "# call the helper function to clean the data\n",
    "dsm = helper.clean_df(dsm, dsm_cols, dsm_labels)\n",
    "\n",
    "# convert cols to numeric\n",
    "dsm = dsm.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# convert values to text strings as follows after the first column\n",
    "# 1 - dependence, 2 - abuse, 3 - no diagnosis, 0 - not present\n",
    "for col in dsm.columns[1:]:\n",
    "    dsm[col] = dsm[col].replace({1:'dependence',2:'abuse',3:'no_diagnosis',0:'not_present'})\n",
    "\n",
    "\n",
    "# fill nulls with 0, where patient does not confirm diagnosis\n",
    "dsm.fillna('not_present', inplace=True)\n",
    "\n",
    "print('Dataframe dsm with shape of', dsm.shape, 'has been cleaned')\n",
    "display(dsm[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform Medical and Psychiatric History Table\n",
    "We will track 18 different medical conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set parameters for transformation\n",
    "mdh_cols = ['patdeid','MDH001','MDH002','MDH003','MDH004','MDH005','MDH006','MDH007','MDH008','MDH009',\n",
    "            'MDH010','MDH011A','MDH011B','MDH012','MDH013','MDH014','MDH015','MDH016','MDH017']\n",
    "mdh_labels = {'MDH001':'mdh_head_injury','MDH002':'mdh_allergies','MDH003':'mdh_liver_problems',\n",
    "                'MDH004':'mdh_kidney_problems','MDH005':'mdh_gi_problems','MDH006':'mdh_thyroid_problems',\n",
    "                'MDH007':'mdh_heart_condition','MDH008':'mdh_asthma','MDH009':'mdh_hypertension',\n",
    "                'MDH010':'mdh_skin_disease','MDH011A':'mdh_opi_withdrawal','MDH011B':'mdh_alc_withdrawal',\n",
    "                'MDH012':'mdh_schizophrenia','MDH013':'mdh_major_depressive_disorder',\n",
    "                'MDH014':'mdh_bipolar_disorder','MDH015':'mdh_anxiety_disorder','MDH016':'mdh_sig_neurological_damage','MDH017':'mdh_epilepsy'}\n",
    "\n",
    "# call the helper function to clean the data\n",
    "mdh = helper.clean_df(mdh, mdh_cols, mdh_labels)\n",
    "\n",
    "# map values to txt strings, 0 = no_history, 1 = yes_history, 9 = not_evaluated, skip the first column\n",
    "for col in mdh.columns[1:]:\n",
    "    mdh[col] = mdh[col].map({0:'no_history', 1:'yes_history', 9:'not_evaluated'})\n",
    "\n",
    "# fill in the nulls, but skip the patdeid column\n",
    "mdh = mdh.fillna('not_evaluated')\n",
    "\n",
    "\n",
    "# visually inspect the data\n",
    "print('Dataframe mdh with shape of', mdh.shape, 'has been cleaned')\n",
    "display(mdh[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform the PEX (Physical Exam) Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set params to clean cols\n",
    "pex_cols = ['patdeid','PEX001A','PEX002A','PEX003A','PEX004A','PEX005A','PEX006A','PEX007A',\n",
    "            'PEX008A','PEX009A','PEX010A','PEX011A','PEX012A','VISIT']\n",
    "pex_labels = {'PEX001A':'pex_gen_appearance','PEX002A':'pex_head_neck','PEX003A':'pex_ears_nose_throat',\n",
    "              'PEX004A':'pex_cardio','PEX005A':'pex_lymph_nodes','PEX006A':'pex_respiratory',\n",
    "              'PEX007A':'pex_musculoskeletal','PEX008A':'pex_gi_system','PEX009A':'pex_extremeties',\n",
    "              'PEX010A':'pex_neurological','PEX011A':'pex_skin','PEX012A':'pex_other'}\n",
    "\n",
    "# this dataset includes data from visit BASELINE and 24, we are only interested in BASELINE\n",
    "pex = pex.loc[pex.VISIT=='BASELINE']\n",
    "              \n",
    "# call the helper function to clean the data\n",
    "pex = helper.clean_df(pex, pex_cols, pex_labels)\n",
    "\n",
    "# map values to strings, 0 = normal, 1 = abnormal, 9 = not_evaluated\n",
    "for col in pex.columns[2:]:\n",
    "    pex[col] = pex[col].map({1:'normal', 2:'abnormal', 9:'not_evaluated'})\n",
    "\n",
    "# imputation strategy: 9 indicates no diagnosis\n",
    "pex.fillna('not_present', inplace=True)\n",
    "\n",
    "# drop the visit column\n",
    "pex.drop(columns='VISIT', inplace=True)\n",
    "\n",
    "# visually inspect the data\n",
    "print('Dataframe pex with shape of', pex.shape, 'has been cleaned')\n",
    "display(pex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform the TFB (Timeline Follow Back Survey) Table\n",
    "- This table has an issue with multiple rows per patient\n",
    "- Each report of drug use is recorded in a new row\n",
    "- We will aggregate the data to a single row per patient\n",
    "- After the aggregation, the table will be flattened, to encode the survey, drug class and week collected, in each column\n",
    "- Surveys are collected once a month and reflect the previous 30 days of drug use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define parameters for cleaning\n",
    "tfb_cols = ['patdeid','VISIT','TFB001A','TFB002A','TFB003A','TFB004A','TFB005A','TFB006A','TFB007A',\n",
    "            'TFB008A','TFB009A','TFB010A']\n",
    "tfb_labels = {'TFB001A':'survey_alcohol','TFB002A':'survey_cannabis','TFB003A':'survey_cocaine',    \n",
    "              'TFB010A':'survey_oxycodone','TFB009A':'survey_mmethadone','TFB004A':'survey_amphetamine','TFB005A':'survey_methamphetamine','TFB006A':'survey_opiates','TFB007A':'survey_benzodiazepines','TFB008A':'survey_propoxyphene'}\n",
    "\n",
    "# call the helper function to clean the data\n",
    "tfb = helper.clean_df(tfb, tfb_cols, tfb_labels)\n",
    "\n",
    "# visually inspect the data\n",
    "print('Shape of cleaned tfb dataframe is', tfb.shape)\n",
    "display(tfb[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregate rows by patient and visit, sum all records of drug use\n",
    "\n",
    "# create index\n",
    "index = ['patdeid','VISIT']\n",
    "\n",
    "# create aggregation dictionary, omit the first two columns, they do not require aggregation\n",
    "agg_dict = {col:'sum' for col in tfb.columns[2:]}\n",
    "\n",
    "# aggregate the data, we will apply sum to all instances of reported us to give the total use for the period\n",
    "tfb_agg = tfb.groupby(index).agg(agg_dict).reset_index()\n",
    "\n",
    "# visually inspect the data\n",
    "print('Aggregated tfb dataframe contains', tfb_agg.shape[0],'rows, coming from', tfb.shape[0],'rows')\n",
    "display(tfb_agg[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten the dataframe\n",
    "\n",
    "# set parameters to flatten survey data\n",
    "start = 0 # include data starting from week 0\n",
    "end = 24 # finish at week 24\n",
    "step = 4 # include data for every 4 weeks\n",
    "\n",
    "# call function to flatten dataframe\n",
    "tfb_flat = helper.flatten_dataframe(tfb_agg, start, end, step)\n",
    "\n",
    "# imputation strategy: fill missing values with 0, indicates no drug use\n",
    "tfb_flat.fillna(0, inplace=True)\n",
    "\n",
    "# visualize the data\n",
    "print('Flattended dataframe contains', tfb_flat.shape[1]-1,'features')\n",
    "display(tfb_flat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform the Medication Dose Table\n",
    "- This table has an issue with multiple rows per patient\n",
    "- Each dose of medication is recorded as a row\n",
    "- This means that if a patient received 7 doses of medication, there will be 7 rows for that patient\n",
    "- This needs to be consolidated into a single row per patient\n",
    "- For total_dose with null values, we will treat that as a no show or 0 dose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set parameters for cleaning the dataframe\n",
    "dos_cols = ['patdeid','VISIT','DOS002','DOS005']   \n",
    "dos_labels = {'DOS002':'medication','DOS005':'total_dose'}\n",
    "\n",
    "# call the helper function to clean the data\n",
    "dos = helper.clean_df(dos, dos_cols, dos_labels)\n",
    "\n",
    "# Imputation strategy: backfill and forwardfill missing values from medication and total dose\n",
    "dos['medication'] = dos['medication'].fillna(method='ffill').fillna(method='bfill')\n",
    "dos['total_dose'] = dos['total_dose'].fillna(method='ffill').fillna(method='bfill')\n",
    "\n",
    "# observe the data\n",
    "print('The medication dataframe contains', dos.shape[0],'rows that must be aggregated')\n",
    "display(dos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregate columns \n",
    "\n",
    "# create index\n",
    "index = ['patdeid','VISIT','medication']\n",
    "# create aggregation dictionary\n",
    "agg_dict = {col:'sum' for col in dos.columns[3:]}\n",
    "\n",
    "# aggregate the data, we will add daily dose to create weekly dose total, aggregating multiple columns per patient\n",
    "dos_agg = dos.groupby(index).agg(agg_dict).reset_index()\n",
    "\n",
    "# create df with patdeid and medication to merge later, this will help make analysis easier\n",
    "medication = dos[['patdeid', 'medication']].drop_duplicates(subset=['patdeid'], keep='first').reset_index(drop=True)\n",
    "\n",
    "# visualize the data\n",
    "print('Total rows in the aggregated dataframe:', dos_agg.shape[0],'from', dos.shape[0],'rows')\n",
    "dos_agg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering\n",
    "Create separate columns for bupe and methadone, this improves data quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature engineering\n",
    "\n",
    "# call helper function to create features from the medication data\n",
    "dos_agg = helper.med_features(dos_agg)\n",
    "\n",
    "# visually inspect the data\n",
    "print('The aggregated dataframe contains', dos_agg.shape[1]-2,'features')\n",
    "display(dos_agg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten the dataframe\n",
    "\n",
    "# set parameters to flatten the dataframe\n",
    "start = 0 # include data starting from week 0\n",
    "end = 24 # finish at week 24\n",
    "step = 1 # include data for every week\n",
    "\n",
    "# call function to flatten dataframe\n",
    "dos_flat = helper.flatten_dataframe(dos_agg, start, end, step)\n",
    "\n",
    "# imputation strategy: nulls come post merge, these were visits for patients who dropped out, fill with 0\n",
    "dos_flat.fillna(0, inplace=True)\n",
    "\n",
    "print('The flattened dataframe contains', dos_flat.shape[1]-1,'features')\n",
    "display(dos_flat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform the Clinical Opiate Withdrawal Scale Table\n",
    "There are 2 files that represent predose and postdose observation for patient withdrawal symptoms<br>\n",
    "There is one column per file with the score, so this is the most simple transformation<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set parameters for cleaning the dataframe\n",
    "cw1_cols = ['patdeid','COWS012']\n",
    "cw1_labels = {'COWS012':'cows_predose'}\n",
    "\n",
    "# call helper function to clean columns\n",
    "cw1 = helper.clean_df(cw1, cw1_cols, cw1_labels)\n",
    "\n",
    "cw1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set parameters for cleaning the dataframe\n",
    "cw2_cols = ['patdeid','COWS012']\n",
    "cw2_labels = {'COWS012':'cows_postdose'}\n",
    "\n",
    "# call helper function to clean columns\n",
    "cw2 = helper.clean_df(cw2, cw2_cols, cw2_labels)\n",
    "cw2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we will merge all the tables into a single dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set parameters for merge\n",
    "\n",
    "# Define the dataframes to merge\n",
    "dfs = [rsa_flat, dos_flat, uds_flat, tfb_flat, \n",
    "       uds_features, dem, dsm, mdh, pex, \n",
    "       medication, attendence, cw1, cw2]\n",
    "\n",
    "# Initialize merged_df with the first DataFrame in the list\n",
    "merged_df = dfs[0]\n",
    "\n",
    "# Merge the dfs above using left merge on 'patdeid'\n",
    "for df in dfs[1:]:  # Start from the second item in the list\n",
    "    merged_df = pd.merge(merged_df, df, on='patdeid', how='left')\n",
    "\n",
    "# some rows were duplicated from one:many merge, they will be dropped\n",
    "merged_df = merged_df.drop_duplicates(subset=['patdeid'], keep='first')\n",
    "\n",
    "# Print the shape of the final dataframe\n",
    "print('The final table includes', merged_df.shape[1]-1, 'features for', merged_df.shape[0], 'patients in treatment')\n",
    "\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze Null Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show all rows from the function call\n",
    "pd.set_option('display.max_rows', None)\n",
    "merged_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputation Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the medication column we will backfill and forward fill the nulls\n",
    "# these patients dropped out, however, we would like to closely track their meds where possible\n",
    "merged_df.medication = merged_df.medication.fillna(method='ffill').fillna(method='bfill')\n",
    "\n",
    "# for columns that have 'meds' in the column name, forwardfill and backfill nulls\n",
    "# these columns are the daily dose of medication\n",
    "for col in merged_df.columns:\n",
    "    if 'meds' in col:\n",
    "        merged_df[col] = merged_df[col].fillna(method='ffill').fillna(method='bfill')\n",
    "\n",
    "# for the sae and pbc columns, the nulls are just patients without data, can be set to 0\n",
    "# for the pex columns, nulls come from patients who dropped from treatment\n",
    "# can be filled with 0\n",
    "\n",
    "# create list with prefix of columns to fill for zero value\n",
    "cols1 = ['survey','pbc','cows']\n",
    "for col in merged_df.columns:\n",
    "    if any(x in col for x in cols1):\n",
    "        merged_df[col] = merged_df[col].fillna(0)\n",
    "\n",
    "# set nulls in mdh to not_evaluated\n",
    "cols2 = ['pex', 'dsm', 'mdh']\n",
    "for col in merged_df.columns:\n",
    "    if any(x in col for x in cols2):\n",
    "        merged_df[col] = merged_df[col].fillna('not_evaluated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete rows for incomplete patient profiles -  334, 1003 and 1006\n",
    "merged_df = merged_df[~merged_df['patdeid'].isin([334, 1003, 1006])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to data folder in csv\n",
    "merged_df.to_csv('../data/merged_data.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
