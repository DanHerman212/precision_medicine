{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "import sklearn\n",
    "import itertools\n",
    "import pydotplus\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import helper\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from IPython.display import Image \n",
    "\n",
    "from sklearn.tree import export_graphviz\n",
    "from six import StringIO\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer, SimpleImputer\n",
    "\n",
    "\n",
    "# We'll also import some helper functions that will be useful later on.\n",
    "#from util import load_data, cindex\n",
    "#from public_tests import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/final_predictors.csv')\n",
    "print('Shape of data:', data.shape)\n",
    "\n",
    "# set the features and labels\n",
    "X, y = data.drop(columns=['dropout']), data['dropout']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the dev and test sets \n",
    "X_dev, X_test, y_dev, y_test = train_test_split(X, y, test_size=0.15, random_state=42)\n",
    "\n",
    "# create the train and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_dev, y_dev, test_size=0.15, random_state=42)\n",
    "\n",
    "# check the shape of the train, validation, and test sets\n",
    "print(X_train.shape, X_val.shape, X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.iloc[10, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier(max_depth=None, random_state=42)\n",
    "dt.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we will evaluate the model using the C-Index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_preds = dt.predict_proba(X_train)[:, 1]\n",
    "print(f\"Train C-Index: {helper.cindex(y_train.values, y_train_preds)}\")\n",
    "\n",
    "y_val_preds = dt.predict_proba(X_val)[:, 1]\n",
    "print(f\"Val C-Index: {helper.cindex(y_val.values, y_val_preds)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update Hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment with different hyperparameters for the DecisionTreeClassifier\n",
    "# until you get a c-index above 0.6 for the validation set\n",
    "dt_hyperparams = {\n",
    "    # set your own hyperparameters below, such as 'min_samples_split': 1\n",
    "\n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    'criterion': 'gini', \n",
    "    'max_depth': 3, \n",
    "    'max_features': None, \n",
    "    'min_samples_leaf': 1, \n",
    "    'min_samples_split': 2\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C2 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "dt_reg = DecisionTreeClassifier(**dt_hyperparams, random_state=10)\n",
    "dt_reg.fit(X_train, y_train)\n",
    "\n",
    "y_train_preds = dt_reg.predict_proba(X_train)[:, 1]\n",
    "y_val_preds = dt_reg.predict_proba(X_val)[:, 1]\n",
    "print(f\"Train C-Index: {helper.cindex(y_train.values, y_train_preds)}\")\n",
    "print(f\"Val C-Index (expected > 0.6): {helper.cindex(y_val.values, y_val_preds)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot_data = StringIO()\n",
    "export_graphviz(dt_reg, feature_names=X_train.columns, out_file=dot_data,  \n",
    "                filled=True, rounded=True, proportion=True, special_characters=True,\n",
    "                impurity=False, class_names=['neg', 'pos'], precision=2)\n",
    "graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n",
    "Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=100, random_state=10)\n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_rf_preds = rf.predict_proba(X_train)[:, 1]\n",
    "print(f\"Train C-Index: {helper.cindex(y_train.values, y_train_rf_preds)}\")\n",
    "\n",
    "y_val_rf_preds = rf.predict_proba(X_val)[:, 1]\n",
    "print(f\"Val C-Index: {helper.cindex(y_val.values, y_val_rf_preds)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def holdout_grid_search(clf, X_train_hp, y_train_hp, X_val_hp, y_val_hp, hyperparams, fixed_hyperparams={}):\n",
    "    '''\n",
    "    Conduct hyperparameter grid search on hold out validation set. Use holdout validation.\n",
    "    Hyperparameters are input as a dictionary mapping each hyperparameter name to the\n",
    "    range of values they should iterate over. Use the cindex function as your evaluation\n",
    "    function.\n",
    "\n",
    "    Input:\n",
    "        clf: sklearn classifier\n",
    "        X_train_hp (dataframe): dataframe for training set input variables\n",
    "        y_train_hp (dataframe): dataframe for training set targets\n",
    "        X_val_hp (dataframe): dataframe for validation set input variables\n",
    "        y_val_hp (dataframe): dataframe for validation set targets\n",
    "        hyperparams (dict): hyperparameter dictionary mapping hyperparameter\n",
    "                            names to range of values for grid search\n",
    "        fixed_hyperparams (dict): dictionary of fixed hyperparameters that\n",
    "                                  are not included in the grid search\n",
    "\n",
    "    Output:\n",
    "        best_estimator (sklearn classifier): fitted sklearn classifier with best performance on\n",
    "                                             validation set\n",
    "        best_hyperparams (dict): hyperparameter dictionary mapping hyperparameter\n",
    "                                 names to values in best_estimator\n",
    "    '''\n",
    "    best_estimator = None\n",
    "    best_hyperparams = {}\n",
    "    \n",
    "    # hold best running score\n",
    "    best_score = 0.0\n",
    "\n",
    "    # get list of param values\n",
    "    lists = hyperparams.values()\n",
    "    \n",
    "    # get all param combinations\n",
    "    param_combinations = list(itertools.product(*lists))\n",
    "    total_param_combinations = len(param_combinations)\n",
    "\n",
    "    # store results for plotting\n",
    "    results = []\n",
    "\n",
    "    # iterate through param combinations\n",
    "    for i, params in enumerate(param_combinations, 1):\n",
    "        # fill param dict with params\n",
    "        param_dict = {}\n",
    "        for param_index, param_name in enumerate(hyperparams):\n",
    "            param_dict[param_name] = params[param_index]\n",
    "            \n",
    "        # create estimator with specified params\n",
    "        estimator = clf(**param_dict, **fixed_hyperparams)\n",
    "\n",
    "        # fit estimator\n",
    "        estimator.fit(X_train_hp, y_train_hp)\n",
    "        \n",
    "        # get predictions on validation set\n",
    "        preds = estimator.predict_proba(X_val_hp)\n",
    "        \n",
    "        # compute cindex for predictions\n",
    "        estimator_score = helper.cindex(y_val_hp, preds[:,1])\n",
    "\n",
    "        print(f'[{i}/{total_param_combinations}] {param_dict}')\n",
    "        print(f'Val C-Index: {estimator_score}\\n')\n",
    "\n",
    "        # store results \n",
    "        results.append([param_dict, estimator_score])\n",
    "\n",
    "        # if new high score, update high score, best estimator\n",
    "        # and best params \n",
    "        if estimator_score >= best_score:\n",
    "                best_score = estimator_score\n",
    "                best_estimator = estimator\n",
    "                best_hyperparams = param_dict\n",
    "\n",
    "    # add fixed hyperparamters to best combination of variable hyperparameters\n",
    "    best_hyperparams.update(fixed_hyperparams)\n",
    "    \n",
    "    return best_estimator, best_hyperparams, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_forest_grid_search(X_train_dropped, y_train_dropped, X_val_dropped, y_val_dropped):\n",
    "\n",
    "    # Define ranges for the chosen random forest hyperparameters \n",
    "    hyperparams = {\n",
    "        \n",
    "        ### START CODE HERE (REPLACE array values with your code) ###\n",
    "\n",
    "        # how many trees should be in the forest (int)\n",
    "        'n_estimators': [100, 200, 300],\n",
    "\n",
    "        # the maximum depth of trees in the forest (int)\n",
    "        \n",
    "        'max_depth': [3, 5, 7],\n",
    "        \n",
    "        # the minimum number of samples in a leaf as a fraction\n",
    "        # of the total number of samples in the training set\n",
    "        # Can be int (in which case that is the minimum number)\n",
    "        # or float (in which case the minimum is that fraction of the\n",
    "        # number of training set samples)\n",
    "        'min_samples_leaf': [0.01, 0.05, 0.1],\n",
    "\n",
    "        ### END CODE HERE ###\n",
    "    }\n",
    "\n",
    "    \n",
    "    fixed_hyperparams = {\n",
    "        'random_state': 10,\n",
    "    }\n",
    "    \n",
    "    rf = RandomForestClassifier\n",
    "\n",
    "    best_rf, best_hyperparams, results = holdout_grid_search(rf, X_train_dropped, y_train_dropped,\n",
    "                                                    X_val_dropped, y_val_dropped, hyperparams,\n",
    "                                                    fixed_hyperparams)\n",
    "\n",
    "    print(f\"Best hyperparameters:\\n{best_hyperparams}\")\n",
    "\n",
    "    \n",
    "    y_train_best = best_rf.predict_proba(X_train_dropped)[:, 1]\n",
    "    print(f\"Train C-Index: {helper.cindex(y_train_dropped, y_train_best)}\")\n",
    "\n",
    "    y_val_best = best_rf.predict_proba(X_val_dropped)[:, 1]\n",
    "    print(f\"Val C-Index: {helper.cindex(y_val_dropped, y_val_best)}\")\n",
    "    \n",
    "    # add fixed hyperparamters to best combination of variable hyperparameters\n",
    "    best_hyperparams.update(fixed_hyperparams)\n",
    "    \n",
    "    return best_rf, best_hyperparams, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_rf, best_hp, results = random_forest_grid_search(X_train, y_train, X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function for xgboost\n",
    "def xgboost_grid_search(X_train_dropped, y_train_dropped, X_val_dropped, y_val_dropped):\n",
    "\n",
    "    # Define ranges for the chosen xgboost hyperparameters \n",
    "    hyperparams = {\n",
    "        \n",
    "        ### START CODE HERE (REPLACE array values with your code) ###\n",
    "\n",
    "        # how many trees should be in the forest (int)\n",
    "        'n_estimators': [100, 200, 300],\n",
    "\n",
    "        # the learning rate of the forest (float)\n",
    "        'learning_rate': [0.01, 0.1, 0.2]\n",
    "\n",
    "        ### END CODE HERE ###\n",
    "    }\n",
    "\n",
    "    \n",
    "    fixed_hyperparams = {\n",
    "        'random_state': 10,\n",
    "    }\n",
    "    \n",
    "    xgb = XGBClassifier\n",
    "\n",
    "    best_xgb, best_hyperparams, results = holdout_grid_search(xgb, X_train_dropped, y_train_dropped,\n",
    "                                                    X_val_dropped, y_val_dropped, hyperparams,\n",
    "                                                    fixed_hyperparams)\n",
    "\n",
    "    print(f\"Best hyperparameters:\\n{best_hyperparams}\")\n",
    "\n",
    "    \n",
    "    y_train_best = best_xgb.predict_proba(X_train_dropped)[:, 1]\n",
    "    print(f\"Train C-Index: {helper.cindex(y_train_dropped, y_train_best)}\")\n",
    "\n",
    "    y_val_best = best_xgb.predict_proba(X_val_dropped)[:, 1]\n",
    "    print(f\"Val C-Index: {helper.cindex(y_val_dropped, y_val_best)}\")\n",
    "    \n",
    "    # add fixed hyperparamters to best combination of variable hyperparameters\n",
    "    best_hyperparams.update(fixed_hyperparams)\n",
    "    \n",
    "    return best_xgb, best_hyperparams, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_xgb, best_hp_xgb, results_xgb = xgboost_grid_search(X_train, y_train, X_val, y_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a fucntion for logistic regression grid search\n",
    "def logistic_reg_grid_search(X_train_hp, y_train_hp, X_val_hp, y_val_hp):\n",
    "\n",
    "    # Define ranges for the chosen logistic regression hyperparameters \n",
    "    hyperparams = {\n",
    "        \n",
    "        ### START CODE HERE (REPLACE array values with your code) ###\n",
    "\n",
    "        # the inverse of the regularization strength\n",
    "        'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "\n",
    "        # the type of norm used in the penalization\n",
    "        'penalty': ['l1', 'l2'],\n",
    "        \n",
    "        # the algorithm to use in the optimization problem\n",
    "        'solver': ['liblinear'],\n",
    "\n",
    "        ### END CODE HERE ###\n",
    "    }\n",
    "\n",
    "    \n",
    "    fixed_hyperparams = {\n",
    "        'random_state': 10,\n",
    "    }\n",
    "    \n",
    "    lr = sklearn.linear_model.LogisticRegression\n",
    "\n",
    "    best_lr, best_hyperparams, results = holdout_grid_search(lr, X_train_hp, y_train_hp,\n",
    "                                                    X_val_hp, y_val_hp, hyperparams,\n",
    "                                                    fixed_hyperparams)\n",
    "\n",
    "    print(f\"Best hyperparameters:\\n{best_hyperparams}\")\n",
    "\n",
    "    \n",
    "    y_train_best = best_lr.predict_proba(X_train_hp)[:, 1]\n",
    "    print(f\"Train C-Index: {helper.cindex(y_train_hp, y_train_best)}\")\n",
    "\n",
    "    y_val_best = best_lr.predict_proba(X_val_hp)[:, 1]\n",
    "    print(f\"Val C-Index: {helper.cindex(y_val_hp, y_val_best)}\")\n",
    "    \n",
    "    # add fixed hyperparamters to best combination of variable hyperparameters\n",
    "    best_hyperparams.update(fixed_hyperparams)\n",
    "    \n",
    "    return best_lr, best_hyperparams, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_lr, best_hp, results = logistic_reg_grid_search(X_train, y_train, X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test for random forest\n",
    "y_test_best = best_rf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(f\"Random Forest C-Index: {helper.cindex(y_test.values, y_test_best)}\")\n",
    "\n",
    "# test for xgboost\n",
    "y_test_best = best_xgb.predict_proba(X_test)[:, 1]\n",
    "print(f\"XGBoost C-Index: {helper.cindex(y_test.values, y_test_best)}\")\n",
    "\n",
    "# test for logistic regression\n",
    "y_test_best = best_lr.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(f\"Logistic Regression C-Index: {helper.cindex(y_test.values, y_test_best)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define ranges for the random forest hyperparameter search \n",
    "hyperparams = {\n",
    "    ### START CODE HERE (REPLACE array values with your code) ###\n",
    "\n",
    "    # how many trees should be in the forest (int)\n",
    "    'n_estimators': [100, 200, 300],\n",
    "\n",
    "    # the maximum depth of trees in the forest (int)\n",
    "    'max_depth': [3, 5, 7],\n",
    "\n",
    "    # the minimum number of samples in a leaf as a fraction\n",
    "    # of the total number of samples in the training set\n",
    "    # Can be int (in which case that is the minimum number)\n",
    "    # or float (in which case the minimum is that fraction of the\n",
    "    # number of training set samples)\n",
    "    'min_samples_leaf': [0.01, 0.5]\n",
    "    ### END CODE HERE ###\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C5 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "rf = RandomForestClassifier\n",
    "\n",
    "best_rf, best_hp , results= holdout_grid_search(rf, X_train, y_train,\n",
    "                                                                     X_val, y_val,\n",
    "                                                                     hyperparams, {'random_state': 10})\n",
    "\n",
    "print(\"Performance for best hyperparameters:\")\n",
    "\n",
    "y_train_best = best_rf.predict_proba(X_train)[:, 1]\n",
    "print(f\"- Train C-Index: {helper.cindex(y_train, y_train_best):.4f}\")\n",
    "\n",
    "y_val_best = best_rf.predict_proba(X_val)[:, 1]\n",
    "print(f\"- Val C-Index: {helper.cindex(y_val, y_val_best):.4f}\")\n",
    "\n",
    "y_test_imp = best_rf.predict_proba(X_test)[:, 1]\n",
    "print(f\"- Test C-Index: {helper.cindex(y_test, y_test_imp):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot confusion matrix\n",
    "def plot_confusion_matrix(y_true, y_pred, labels):\n",
    "    '''\n",
    "    Plot confusion matrix using heatmap.\n",
    "    \n",
    "    Args:\n",
    "    y_true: true labels\n",
    "    y_pred: predicted labels\n",
    "    labels: class labels\n",
    "    '''\n",
    "    cm = sklearn.metrics.confusion_matrix(y_true, y_pred)\n",
    "    fig, ax = plt.subplots(figsize=(5, 5))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', xticklabels=labels, yticklabels=labels, cmap='Blues', cbar=False)\n",
    "    plt.ylabel('Actual')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.title('Confusion matrix')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(y_test, y_test_imp > 0.5, ['No Dropout', 'Dropout']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a shap explainer for random forrest\n",
    "explainer = shap.TreeExplainer(best_rf.predict, X_test)\n",
    "shap_values = explainer.shap_values(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create shap summary plot\n",
    "shap.summary_plot(shap_values, X_test, plot_type='bar')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
